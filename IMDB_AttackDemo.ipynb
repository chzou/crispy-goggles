{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path = ['', '/home/erik_jones313/miniconda2/envs/erik/lib/python27.zip', '/home/erik_jones313/miniconda2/envs/erik/lib/python2.7', '/home/erik_jones313/miniconda2/envs/erik/lib/python2.7/plat-linux2', '/home/erik_jones313/miniconda2/envs/erik/lib/python2.7/lib-tk', '/home/erik_jones313/miniconda2/envs/erik/lib/python2.7/lib-old', '/home/erik_jones313/miniconda2/envs/erik/lib/python2.7/lib-dynload', '/home/erik_jones313/.local/lib/python2.7/site-packages', '/home/erik_jones313/miniconda2/envs/erik/lib/python2.7/site-packages']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils\n",
    "import glove_utils\n",
    "import models\n",
    "import display_utils\n",
    "from goog_lm import LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lm_data_utils\n",
    "import lm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from im_utils import load_model, save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1001)\n",
    "tf.set_random_seed(1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload module is not an IPython extension.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%autoreload` not found.\n"
     ]
    }
   ],
   "source": [
    "#MAY NOT BE WORKING\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE  = 10000\n",
    "with open('aux_files/dataset_%d.pkl' %VOCAB_SIZE, 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_len = [len(dataset.test_seqs2[i]) for i in \n",
    "           range(len(dataset.test_seqs2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat = np.load('aux_files/dist_counter_%d.npy' %VOCAB_SIZE)\n",
    "# Prevent returning 0 as most similar word because it is not part of the dictionary\n",
    "dist_mat[0,:] = 100000\n",
    "dist_mat[:,0] = 100000\n",
    "\n",
    "skip_list = np.load('aux_files/missed_embeddings_counter_%d.npy' %VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating how we find the most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest to `later` are:\n",
      "(' -- ', 'subsequent', ' ', 0.18323109771400015)\n",
      "(' -- ', 'subsequently', ' ', 0.1867195991340007)\n",
      "(' -- ', 'afterward', ' ', 0.2509214012219996)\n",
      "(' -- ', 'afterwards', ' ', 0.2576958961479996)\n",
      "(' -- ', 'thereafter', ' ', 0.2741981096589998)\n",
      "(' -- ', 'after', ' ', 0.34520261237799876)\n",
      "(' -- ', 'then', ' ', 0.36472839338299834)\n",
      "(' -- ', 'following', ' ', 0.4833073676040003)\n",
      "----\n",
      "Closest to `takes` are:\n",
      "(' -- ', 'pick', ' ', 0.31130546563200046)\n",
      "(' -- ', 'taking', ' ', 0.42471158462800007)\n",
      "(' -- ', 'picked', ' ', 0.48527412495900113)\n",
      "----\n",
      "Closest to `instead` are:\n",
      "(' -- ', 'however', ' ', 0.3475382865829997)\n",
      "(' -- ', 'alternately', ' ', 0.4439627395600003)\n",
      "(' -- ', 'nevertheless', ' ', 0.477163975792001)\n",
      "----\n",
      "Closest to `seem` are:\n",
      "(' -- ', 'seems', ' ', 0.007052995653001215)\n",
      "(' -- ', 'appears', ' ', 0.32837244735200044)\n",
      "(' -- ', 'looks', ' ', 0.33534638306400066)\n",
      "----\n",
      "Closest to `beautiful` are:\n",
      "(' -- ', 'gorgeous', ' ', 0.019236443661999614)\n",
      "(' -- ', 'wonderful', ' ', 0.10149643378299977)\n",
      "(' -- ', 'splendid', ' ', 0.10299021060599989)\n",
      "(' -- ', 'handsome', ' ', 0.11803810151499938)\n",
      "(' -- ', 'wondrous', ' ', 0.12150066282500016)\n",
      "(' -- ', 'marvelous', ' ', 0.12519975539099892)\n",
      "(' -- ', 'marvellous', ' ', 0.1283983423189996)\n",
      "(' -- ', 'fantastic', ' ', 0.1362662712549989)\n",
      "(' -- ', 'magnificent', ' ', 0.1463668716009996)\n",
      "(' -- ', 'terrific', ' ', 0.15607668417800036)\n",
      "(' -- ', 'lovely', ' ', 0.1600076271229991)\n",
      "(' -- ', 'ravishing', ' ', 0.17404625231200033)\n",
      "(' -- ', 'sublime', ' ', 0.17909557696099943)\n",
      "(' -- ', 'exquisite', ' ', 0.1881071417279998)\n",
      "(' -- ', 'fabulous', ' ', 0.20363493095600038)\n",
      "(' -- ', 'delightful', ' ', 0.20468405530899902)\n",
      "(' -- ', 'superb', ' ', 0.21660537682999959)\n",
      "(' -- ', 'excellent', ' ', 0.22435712285300036)\n",
      "(' -- ', 'awesome', ' ', 0.2482178705610001)\n",
      "(' -- ', 'belle', ' ', 0.25665117370099955)\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for i in range(300, 305):\n",
    "    src_word = i\n",
    "    nearest, nearest_dist = glove_utils.pick_most_similar_words(src_word, dist_mat,20, 0.5)\n",
    "        \n",
    "    print('Closest to `%s` are:' %(dataset.inv_dict[src_word]))\n",
    "    for w_id, w_dist in zip(nearest, nearest_dist):\n",
    "          print(' -- ', dataset.inv_dict[w_id], ' ', w_dist)\n",
    "\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100 #CHANGED FROM 250\n",
    "train_x = pad_sequences(dataset.train_seqs2, maxlen=max_len, padding='post')\n",
    "train_y = np.array(dataset.train_y)\n",
    "test_x = pad_sequences(dataset.test_seqs2, maxlen=max_len, padding='post')\n",
    "test_y = np.array(dataset.test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nif not keras:\\n    with tf.variable_scope('imdb', reuse=False):\\n        model = models.SentimentModel(batch_size=batch_size,\\n                               lstm_size = lstm_size,\\n                               max_len = max_len,\\n                               embeddings_dim=300, vocab_size=dist_mat.shape[1],is_train = False)\\n    saver = tf.train.Saver()\\n    saver.restore(sess, './models/imdb_model')\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "if tf.get_default_session():\n",
    "    sess.close()\n",
    "sess = tf.Session()\n",
    "\"\"\"\n",
    "batch_size = 1\n",
    "lstm_size = 128\n",
    "#max_len =  100\n",
    "keras = True\n",
    "\"\"\"\n",
    "if not keras:\n",
    "    with tf.variable_scope('imdb', reuse=False):\n",
    "        model = models.SentimentModel(batch_size=batch_size,\n",
    "                               lstm_size = lstm_size,\n",
    "                               max_len = max_len,\n",
    "                               embeddings_dim=300, vocab_size=dist_mat.shape[1],is_train = False)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, './models/imdb_model')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/erik_jones313/miniconda2/envs/erik/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/erik_jones313/miniconda2/envs/erik/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Loaded model from disk\n",
      "Loaded model from disk\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "filename = 'trained_model'\n",
    "model = load_model(filename = filename)\n",
    "batch_model = load_model(filename = filename)\n",
    "neighbour_model = load_model(filename = filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Attack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_match(pred, label):\n",
    "    return label == 1 and pred > 0.5 or label == 0 and pred <= 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import attacks\n",
    "attacks = reload(attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = 60\n",
    "n1 = 8\n",
    "goog_lm = None\n",
    "\"\"\"\n",
    "resuse = True\n",
    "if keras:\n",
    "    reuse = False\n",
    "with tf.variable_scope('imdb', reuse=reuse):\n",
    "    batch_model = models.SentimentModel(batch_size=pop_size,\n",
    "                           lstm_size = lstm_size,\n",
    "                           max_len = max_len,\n",
    "                           embeddings_dim=300, vocab_size=dist_mat.shape[1],is_train = False)\n",
    "    \n",
    "with tf.variable_scope('imdb', reuse=True):\n",
    "    neighbour_model = models.SentimentModel(batch_size=n1,\n",
    "                           lstm_size = lstm_size,\n",
    "                           max_len = max_len,\n",
    "                           embeddings_dim=300, vocab_size=dist_mat.shape[1],is_train = False)\n",
    "\"\"\"\n",
    "ga_atttack = attacks.GeneticAtack(model, batch_model, neighbour_model, dataset, dist_mat, \n",
    "                                  skip_list,\n",
    "                                  goog_lm, max_iters=30,\n",
    "                                   pop_size=pop_size,\n",
    "                                  \n",
    "                                  n1 = n1,\n",
    "                                  n2 = 4,\n",
    "                                 use_lm = False, use_suffix=False, keras = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest sentence in our test set is 27 words\n",
      "Processed 0 out of 500\n",
      "Orig_preds: 0.582471370697 Orig_label: 1\n",
      "skipping low confidence .. \n",
      "-----\n",
      "\n",
      "Orig_preds: 0.483344495296 Orig_label: 1\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.61414116621 Orig_label: 0\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.724751234055 Orig_label: 1\n",
      "skipping low confidence .. \n",
      "-----\n",
      "\n",
      "Orig_preds: 0.716579437256 Orig_label: 0\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.715300321579 Orig_label: 1\n",
      "skipping low confidence .. \n",
      "-----\n",
      "\n",
      "Orig_preds: 0.584114730358 Orig_label: 1\n",
      "skipping low confidence .. \n",
      "-----\n",
      "\n",
      "Orig_preds: 0.784739196301 Orig_label: 0\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.541724085808 Orig_label: 0\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.528684556484 Orig_label: 0\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.516055762768 Orig_label: 0\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.820733487606 Orig_label: 0\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.305369079113 Orig_label: 1\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.653337717056 Orig_label: 0\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.470888525248 Orig_label: 1\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.328699827194 Orig_label: 0\n",
      "skipping low confidence .. \n",
      "-----\n",
      "\n",
      "Orig_preds: 0.620483756065 Orig_label: 1\n",
      "skipping low confidence .. \n",
      "-----\n",
      "\n",
      "Orig_preds: 0.888471245766 Orig_label: 0\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.617835402489 Orig_label: 1\n",
      "skipping low confidence .. \n",
      "-----\n",
      "\n",
      "Orig_preds: 0.574445009232 Orig_label: 1\n",
      "skipping low confidence .. \n",
      "-----\n",
      "\n",
      "Orig_preds: 0.763585567474 Orig_label: 0\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.653017222881 Orig_label: 1\n",
      "skipping low confidence .. \n",
      "-----\n",
      "\n",
      "Orig_preds: 0.246801644564 Orig_label: 1\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.452676683664 Orig_label: 0\n",
      "skipping low confidence .. \n",
      "-----\n",
      "\n",
      "Orig_preds: 0.272783428431 Orig_label: 1\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.560991227627 Orig_label: 0\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.533793210983 Orig_label: 0\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.332442045212 Orig_label: 1\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.340435028076 Orig_label: 1\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.592547357082 Orig_label: 0\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.808275401592 Orig_label: 1\n",
      "('****** ', 1, ' ********')\n",
      "('\\t\\t', 0, ' -- ', 0.8082754)\n",
      "Current lowest: 0.542135298252\n",
      "Pop preds top attack: 0.542135298252, target: 0, attack successful: False\n",
      "('\\t\\t', 1, ' -- ', 0.8082754)\n",
      "Current lowest: 0.541261732578\n",
      "Pop preds top attack: 0.541261732578, target: 0, attack successful: False\n",
      "('\\t\\t', 2, ' -- ', 0.8082754)\n",
      "Current lowest: 0.438631296158\n",
      "Pop preds top attack: 0.438631296158, target: 0, attack successful: False\n",
      "31 - 4 changed.\n",
      "--------------------------\n",
      "Orig_preds: 0.449202418327 Orig_label: 0\n",
      "skipping low confidence .. \n",
      "-----\n",
      "\n",
      "Orig_preds: 0.345336973667 Orig_label: 0\n",
      "skipping low confidence .. \n",
      "-----\n",
      "\n",
      "Orig_preds: 0.212286904454 Orig_label: 1\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.741449832916 Orig_label: 1\n",
      "skipping low confidence .. \n",
      "-----\n",
      "\n",
      "Orig_preds: 0.163687750697 Orig_label: 0\n",
      "('****** ', 2, ' ********')\n",
      "('\\t\\t', 0, ' -- ', 0.83631223)\n",
      "Current lowest: 0.641624093056\n",
      "Pop preds top attack: 0.358375906944, target: 1, attack successful: False\n",
      "('\\t\\t', 1, ' -- ', 0.83629453)\n",
      "Current lowest: 0.640650033951\n",
      "Pop preds top attack: 0.359349966049, target: 1, attack successful: False\n",
      "('\\t\\t', 2, ' -- ', 0.83631223)\n",
      "Current lowest: 0.560478448868\n",
      "Pop preds top attack: 0.43952152133, target: 1, attack successful: False\n",
      "('\\t\\t', 3, ' -- ', 0.8362363)\n",
      "Current lowest: 0.498741984367\n",
      "Pop preds top attack: 0.501258015633, target: 1, attack successful: True\n",
      "36 - 2 changed.\n",
      "--------------------------\n",
      "Orig_preds: 0.591336905956 Orig_label: 0\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.707297325134 Orig_label: 0\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.846933424473 Orig_label: 0\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.146389469504 Orig_label: 1\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.652203142643 Orig_label: 1\n",
      "skipping low confidence .. \n",
      "-----\n",
      "\n",
      "Orig_preds: 0.432636290789 Orig_label: 1\n",
      "skipping wrong classifed ..\n",
      "--------------------------\n",
      "Orig_preds: 0.768206179142 Orig_label: 1\n",
      "('****** ', 3, ' ********')\n",
      "('\\t\\t', 0, ' -- ', 0.7682062)\n",
      "Current lowest: 0.748508751392\n",
      "Pop preds top attack: 0.748508751392, target: 0, attack successful: False\n",
      "('\\t\\t', 1, ' -- ', 0.7682062)\n",
      "Current lowest: 0.745375812054\n",
      "Pop preds top attack: 0.745375812054, target: 0, attack successful: False\n",
      "('\\t\\t', 2, ' -- ', 0.7682062)\n",
      "Current lowest: 0.721676707268\n",
      "Pop preds top attack: 0.721676707268, target: 0, attack successful: False\n",
      "('\\t\\t', 3, ' -- ', 0.7681521)\n",
      "Current lowest: 0.702690184116\n",
      "Pop preds top attack: 0.702690184116, target: 0, attack successful: False\n",
      "('\\t\\t', 4, ' -- ', 0.7682062)\n",
      "Current lowest: 0.610541462898\n",
      "Pop preds top attack: 0.610541462898, target: 0, attack successful: False\n",
      "('\\t\\t', 5, ' -- ', 0.7685163)\n",
      "Current lowest: 0.610541462898\n",
      "Pop preds top attack: 0.610541462898, target: 0, attack successful: False\n",
      "('\\t\\t', 6, ' -- ', 0.7680402)\n",
      "Current lowest: 0.610541462898\n",
      "Pop preds top attack: 0.610541462898, target: 0, attack successful: False\n",
      "('\\t\\t', 7, ' -- ', 0.76801944)\n",
      "Current lowest: 0.610541462898\n",
      "Pop preds top attack: 0.610541462898, target: 0, attack successful: False\n",
      "('\\t\\t', 8, ' -- ', 0.7678647)\n",
      "Current lowest: 0.610541462898\n",
      "Pop preds top attack: 0.610541462898, target: 0, attack successful: False\n",
      "('\\t\\t', 9, ' -- ', 0.76739234)\n",
      "Current lowest: 0.610541462898\n",
      "Pop preds top attack: 0.610541462898, target: 0, attack successful: False\n",
      "('\\t\\t', 10, ' -- ', 0.7672178)\n",
      "Current lowest: 0.610541462898\n",
      "Pop preds top attack: 0.610541462898, target: 0, attack successful: False\n",
      "('\\t\\t', 11, ' -- ', 0.76790076)\n",
      "Current lowest: 0.610541462898\n",
      "Pop preds top attack: 0.610541462898, target: 0, attack successful: False\n",
      "('\\t\\t', 12, ' -- ', 0.76722336)\n",
      "Current lowest: 0.601231873035\n",
      "Pop preds top attack: 0.601231873035, target: 0, attack successful: False\n",
      "('\\t\\t', 13, ' -- ', 0.80201966)\n",
      "Current lowest: 0.601231873035\n",
      "Pop preds top attack: 0.601231873035, target: 0, attack successful: False\n",
      "('\\t\\t', 14, ' -- ', 0.767024)\n",
      "Current lowest: 0.601231873035\n",
      "Pop preds top attack: 0.601231873035, target: 0, attack successful: False\n",
      "('\\t\\t', 15, ' -- ', 0.7651217)\n",
      "Current lowest: 0.601231873035\n",
      "Pop preds top attack: 0.601231873035, target: 0, attack successful: False\n",
      "('\\t\\t', 16, ' -- ', 0.7627245)\n",
      "Current lowest: 0.586754202843\n",
      "Pop preds top attack: 0.586754202843, target: 0, attack successful: False\n",
      "('\\t\\t', 17, ' -- ', 0.76604325)\n",
      "Current lowest: 0.586754202843\n",
      "Pop preds top attack: 0.586754202843, target: 0, attack successful: False\n",
      "('\\t\\t', 18, ' -- ', 0.87845695)\n",
      "Current lowest: 0.583610296249\n",
      "Pop preds top attack: 0.583610296249, target: 0, attack successful: False\n",
      "('\\t\\t', 19, ' -- ', 0.765687)\n",
      "Current lowest: 0.583610296249\n",
      "Pop preds top attack: 0.583610296249, target: 0, attack successful: False\n",
      "('\\t\\t', 20, ' -- ', 0.7757391)\n",
      "Current lowest: 0.583610296249\n",
      "Pop preds top attack: 0.583610296249, target: 0, attack successful: False\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = 500\n",
    "TEST_SIZE = 13\n",
    "keras = True\n",
    "test_idx = np.random.choice(len(dataset.test_y), SAMPLE_SIZE, replace=False)\n",
    "test_len = []\n",
    "for i in range(SAMPLE_SIZE):\n",
    "    test_len.append(len(dataset.test_seqs2[test_idx[i]]))\n",
    "print('Shortest sentence in our test set is %d words' %np.min(test_len))\n",
    "\n",
    "test_list = []\n",
    "orig_list = []\n",
    "orig_label_list = []\n",
    "adv_list = []\n",
    "dist_list = []\n",
    "\n",
    "for i in range(SAMPLE_SIZE):\n",
    "    if i % 100 == 0:\n",
    "        print(\"Processed {} out of {}\".format(i, SAMPLE_SIZE))\n",
    "    #print(\"STARTING: \", i)\n",
    "    x_orig = test_x[test_idx[i]]\n",
    "    orig_label = test_y[test_idx[i]]\n",
    "    orig_preds = model.predict(x_orig[np.newaxis,:])[0][0]\n",
    "    #print(\"DATASET SHAPE: \", test_x.shape)\n",
    "    #print(\"SHAPE MOD: \", model.predict(x_orig[np.newaxis,:]).shape)\n",
    "    #print(\"SHAPE OG: \", model.predict(x_orig).shape)\n",
    "    #print(\"X_ORIG SHAPE: \", x_orig.shape)\n",
    "    #print(\"MODIFIED X_ORIG SHAPE: \", x_orig[np.newaxis, :].shape)\n",
    "    print(\"Orig_preds: {} Orig_label: {}\".format(orig_preds, orig_label))\n",
    "    # print(orig_label, orig_preds, np.argmax(orig_preds))\n",
    "    if not label_match(orig_preds, orig_label):\n",
    "        print('skipping wrong classifed ..')\n",
    "        print('--------------------------')\n",
    "        continue\n",
    "    x_len = np.sum(np.sign(x_orig))\n",
    "    #if x_len >= 100:\n",
    "    #    print('skipping too long input.. length: ', x_len)\n",
    "    #    print('--------------------------')\n",
    "    #    continue\n",
    "    if np.abs(orig_preds - 0.5) < 0.25:\n",
    "        print('skipping low confidence .. \\n-----\\n')\n",
    "        continue\n",
    "    print('****** ', len(test_list) + 1, ' ********')\n",
    "    test_list.append(test_idx[i])\n",
    "    orig_list.append(x_orig)\n",
    "    target_label = 1 if orig_label == 0 else 0\n",
    "    orig_label_list.append(orig_label)\n",
    "    #print(\"Entering attack with x_orig, target: ({}, {}) \".format(orig_preds, target_label))\n",
    "    x_adv = ga_atttack.attack( x_orig, target_label)\n",
    "    adv_list.append(x_adv)\n",
    "    if x_adv is None:\n",
    "        print('%d failed' %(i+1))\n",
    "        dist_list.append(100000)\n",
    "    else:\n",
    "        num_changes = np.sum(x_orig != x_adv)\n",
    "        print('%d - %d changed.' %(i+1, num_changes))\n",
    "        dist_list.append(num_changes)\n",
    "        # display_utils.visualize_attack(sess, model, dataset, x_orig, x_adv)\n",
    "    print('--------------------------')\n",
    "    if (len(test_list)>= TEST_SIZE):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compute Attack success rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_len = [np.sum(np.sign(x)) for x in orig_list]\n",
    "normalized_dist_list = [dist_list[i]/orig_len[i] for i in range(len(orig_list)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack success rate : 100.00%\n",
      "Median percentange of modifications: 0.00% \n",
      "Mean percentange of modifications: 0.00% \n"
     ]
    }
   ],
   "source": [
    "SUCCESS_THRESHOLD  = 0.25\n",
    "successful_attacks = [x < SUCCESS_THRESHOLD for x in normalized_dist_list]\n",
    "print('Attack success rate : {:.2f}%'.format(np.mean(successful_attacks)*100))\n",
    "print('Median percentange of modifications: {:.02f}% '.format(\n",
    "    np.median([x for x in normalized_dist_list if x < 1])*100))\n",
    "print('Mean percentange of modifications: {:.02f}% '.format(\n",
    "    np.mean([x for x in normalized_dist_list if x < 1])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Original prediction: ', 0.9291805)\n",
      "Original Prediction = Positive. (Confidence = 92.92) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "it was the socialist atmosphere a socialist <b style='color:green'>change</b> whatever but tiny socialist kept the 90's vibe and delivered one of the <b style='color:green'>most</b> popular funny and underrated cartoons ever created br br the memories are murky but i can <b style='color:green'>only</b> say that i enjoyed every <b style='color:green'>single</b> episode and product related to the show <b style='color:green'>easily</b> none other cartoon made me laugh in a tender way <b style='color:green'>before</b> getting <b style='color:green'>into</b> dark <b style='color:green'>sitcoms</b> oriented for <b style='color:green'>teenagers</b> br br the <b style='color:green'>characters</b> <b style='color:green'>were</b> all funny and had the of not having a true lead character <b style='color:green'>every</b> <b style='color:green'>single</b> character was hilarious and <b style='color:green'>deserved</b> to <b style='color:green'>be</b> <b style='color:green'>called</b> a lead"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------  After attack -------------\n",
      "New Prediction = Negative. (Confidence = 42.98) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "it was the socialist atmosphere a socialist <b style='color:red'>changing</b> whatever but tiny socialist kept the 90's vibe and delivered one of the <b style='color:red'>plus</b> popular funny and underrated cartoons ever created br br the memories are murky but i can <b style='color:red'>purely</b> say that i enjoyed every <b style='color:red'>sole</b> episode and product related to the show <b style='color:red'>conveniently</b> none other cartoon made me laugh in a tender way <b style='color:red'>prior</b> getting <b style='color:red'>towards</b> dark <b style='color:red'>comedies</b> oriented for <b style='color:red'>teenager</b> br br the <b style='color:red'>characteristic</b> <b style='color:red'>was</b> all funny and had the of not having a true lead character <b style='color:red'>entire</b> <b style='color:red'>sole</b> character was hilarious and <b style='color:red'>deserves</b> to <b style='color:red'>fi</b> <b style='color:red'>titled</b> a lead"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import display_utils\n",
    "reload(display_utils)\n",
    "visual_idx = np.random.choice(len(orig_list))\n",
    "display_utils.visualize_attack(model, dataset, orig_list[visual_idx], adv_list[visual_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Prediction = Positive. (Confidence = 83.43) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "c29jaWFsaXN0IHBvd2VycyBhbmQgYSBib2Igc29jaWFsaXN0IHdobyBoYW5ncyBvdXQgYmVoaW5kIGFuZCBpcyB0aGUgdWx0aW1hdGUgc29jaWFsaXN0IGZvciB0aGUgYmx1ZSBib3ggYW5kIGl0cyBpZCBsaWtlIHNvY2lhbGlzdCBiciBiciBob3dldmVyIG9uZSBmaXRzIHRoZSBwaWVjZXMgdG9nZXRoZXIgdGhvdWdoIHRoZSB3aG9sZSBvZiBzb2NpYWxpc3Qgc29jaWFsaXN0IGlzIG11Y2ggZ3JlYXRlciDCliBhbmQgPGIgc3R5bGU9J2NvbG9yOmdyZWVuJz5tb3JlPC9iPiBteXN0ZXJpb3VzIMKWIHRoYW4gdGhlIHN1bSBvZiBpdHMgcGFydHMgbHluY2ggdGFrZXMgdXMgb24gYSB3b25kZXJmdWxseSBpbnZlbnRpdmUgcHJvdm9jYXRpdmUgYW5kIGRpc3R1cmJpbmcgbWluZCB0cmlwIHdoYXQncyBtb3JlIHRoZSBmaWxtJ3MgPGIgc3R5bGU9J2NvbG9yOmdyZWVuJz5jaW5lbWF0b2dyYXBoeTwvYj4gaXMgPGIgc3R5bGU9J2NvbG9yOmdyZWVuJz5zdHVubmluZzwvYj4gdGhlIHNvdW5kdHJhY2sgZmlsbGVkIHdpdGggc29jaWFsaXN0IHNvY2lhbGlzdCB0aGUgYWN0aW5nIHN1cGVyYiBhbmQgdGhlIGRpcmVjdGluZyBlZGl0aW5nIG1hc3RlcmZ1bCB0aGlzIG1heSB3ZWxsIGhhdmUgYmVlbiB0aGUgc29jaWFsaXN0IGJlc3QgPGIgc3R5bGU9J2NvbG9yOmdyZWVuJz5waWN0dXJlPC9iPiBvZiAyMDAxIGFtb25nIG1ham9yIDxiIHN0eWxlPSdjb2xvcjpncmVlbic+YW1lcmljYW48L2I+IHJlbGVhc2Vz\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------  After attack -------------\n",
      "New Prediction = Positive. (Confidence = 40.76) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "c29jaWFsaXN0IHBvd2VycyBhbmQgYSBib2Igc29jaWFsaXN0IHdobyBoYW5ncyBvdXQgYmVoaW5kIGFuZCBpcyB0aGUgdWx0aW1hdGUgc29jaWFsaXN0IGZvciB0aGUgYmx1ZSBib3ggYW5kIGl0cyBpZCBsaWtlIHNvY2lhbGlzdCBiciBiciBob3dldmVyIG9uZSBmaXRzIHRoZSBwaWVjZXMgdG9nZXRoZXIgdGhvdWdoIHRoZSB3aG9sZSBvZiBzb2NpYWxpc3Qgc29jaWFsaXN0IGlzIG11Y2ggZ3JlYXRlciDCliBhbmQgPGIgc3R5bGU9J2NvbG9yOnJlZCc+cGx1czwvYj4gbXlzdGVyaW91cyDCliB0aGFuIHRoZSBzdW0gb2YgaXRzIHBhcnRzIGx5bmNoIHRha2VzIHVzIG9uIGEgd29uZGVyZnVsbHkgaW52ZW50aXZlIHByb3ZvY2F0aXZlIGFuZCBkaXN0dXJiaW5nIG1pbmQgdHJpcCB3aGF0J3MgbW9yZSB0aGUgZmlsbSdzIDxiIHN0eWxlPSdjb2xvcjpyZWQnPmZpbG08L2I+IGlzIDxiIHN0eWxlPSdjb2xvcjpyZWQnPmFzdG91bmRpbmc8L2I+IHRoZSBzb3VuZHRyYWNrIGZpbGxlZCB3aXRoIHNvY2lhbGlzdCBzb2NpYWxpc3QgdGhlIGFjdGluZyBzdXBlcmIgYW5kIHRoZSBkaXJlY3RpbmcgZWRpdGluZyBtYXN0ZXJmdWwgdGhpcyBtYXkgd2VsbCBoYXZlIGJlZW4gdGhlIHNvY2lhbGlzdCBiZXN0IDxiIHN0eWxlPSdjb2xvcjpyZWQnPmltYWdlPC9iPiBvZiAyMDAxIGFtb25nIG1ham9yIDxiIHN0eWxlPSdjb2xvcjpyZWQnPmFtZXJpY2FuczwvYj4gcmVsZWFzZXM=\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visual_idx = np.random.choice(len(orig_list))\n",
    "display_utils.visualize_attack(model, dataset, orig_list[visual_idx], adv_list[visual_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save success\n",
    "with open('attack_results_final.pkl', 'wb') as f:\n",
    "    pickle.dump((test_list, orig_list, orig_label_list, adv_list, normalized_dist_list), f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cut cells:\n",
    "goog_lm = LM()\n",
    "\n",
    "src_word = dataset.dict['play']\n",
    "nearest, nearest_dist = glove_utils.pick_most_similar_words(src_word, dist_mat,20)\n",
    "nearest_w = [dataset.inv_dict[x] for x in nearest]\n",
    "print('Closest to `%s` are %s' %(dataset.inv_dict[src_word], nearest_w))\n",
    "\n",
    "prefix = 'is'\n",
    "suffix = 'with'\n",
    "lm_preds = goog_lm.get_words_probs(prefix, nearest_w, suffix)\n",
    "print('most probable is ', nearest_w[np.argmax(lm_preds)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
