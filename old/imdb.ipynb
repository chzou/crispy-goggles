{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/aclImdb/imdb.vocab', 'r') as f:\n",
    "    vocab = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LEN = len(vocab)\n",
    "NUM_CLASSES = 10\n",
    "NUM_EXAMPLES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(n):\n",
    "    arr = np.zeros(NUM_CLASSES)\n",
    "    arr[int(n)-1] = 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove2dict(src_filename):\n",
    "    data = {}\n",
    "    with open(src_filename) as f:\n",
    "        while True:\n",
    "            try:\n",
    "                line = next(f)\n",
    "                line = line.strip().split()\n",
    "                data[line[0]] = np.array(line[1: ], dtype=np.float)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "glove_dim = 300\n",
    "GLOVE = glove2dict('./data/glove.6B/glove.6B.{}d.txt'.format(glove_dim))\n",
    "\n",
    "def randvec(n=50, lower=-1.0, upper=1.0):\n",
    "    \"\"\"Returns a random vector of length `n`. `w` is ignored.\"\"\"\n",
    "    return np.array([random.uniform(lower, upper) for i in range(n)])\n",
    "\n",
    "def glove_vec(w):\n",
    "    \"\"\"Return `w`'s GloVe representation if available, else return \n",
    "    a random vector.\"\"\"\n",
    "    return GLOVE.get(w, randvec(glove_dim))\n",
    "\n",
    "def vec_average(u, v):\n",
    "    \"\"\"Averages np.array instances `u` and `v` into a new np.array\"\"\"\n",
    "    return np.add(u, v) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_to_vec(features):\n",
    "    all_word_vecs = []\n",
    "    for f in features:\n",
    "        i, c = f.split(':')    # index, count\n",
    "        w = vocab[int(i)]      # get vocab word by index\n",
    "        g = glove_vec(w)       # glove word embedding\n",
    "        all_word_vecs.append(g)\n",
    "    output = np.mean(all_word_vecs, axis=0)    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_data(filename, num_examples):\n",
    "    with open(filename, 'r') as f:\n",
    "        imdb = f.readlines()\n",
    "    \n",
    "    x_train, y_train = [], []\n",
    "    label_count = defaultdict(int) # used to balance dataset\n",
    "    for line in imdb:\n",
    "        label, *features = line.split(' ')\n",
    "        if label_count[label] >= NUM_EXAMPLES / NUM_CLASSES:\n",
    "            continue\n",
    "        x_train.append(bow_to_vec(features))\n",
    "        y_train.append(int(label) - 1)\n",
    "        label_count[label] += 1\n",
    "    \n",
    "    x_train = torch.tensor(x_train, dtype=torch.float)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_smol, y_train_smol = get_data('./data/aclImdb/train/labeledBow.feat', NUM_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in, n_h, n_out = glove_dim, 500, NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(n_in, n_h),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(n_h, n_out),\n",
    "                     nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  2.30368709564209\n",
      "epoch:  1  loss:  2.2481884956359863\n",
      "epoch:  2  loss:  2.2100918292999268\n",
      "epoch:  3  loss:  2.1869142055511475\n",
      "epoch:  4  loss:  2.1751456260681152\n",
      "epoch:  5  loss:  2.1700966358184814\n",
      "epoch:  6  loss:  2.1681807041168213\n",
      "epoch:  7  loss:  2.1675074100494385\n",
      "epoch:  8  loss:  2.1673009395599365\n",
      "epoch:  9  loss:  2.1672537326812744\n",
      "epoch:  10  loss:  2.1672582626342773\n",
      "epoch:  11  loss:  2.1672534942626953\n",
      "epoch:  12  loss:  2.167214870452881\n",
      "epoch:  13  loss:  2.167201042175293\n",
      "epoch:  14  loss:  2.1671903133392334\n",
      "epoch:  15  loss:  2.1671650409698486\n",
      "epoch:  16  loss:  2.167114734649658\n",
      "epoch:  17  loss:  2.1670045852661133\n",
      "epoch:  18  loss:  2.166792392730713\n",
      "epoch:  19  loss:  2.166329860687256\n",
      "epoch:  20  loss:  2.1653075218200684\n",
      "epoch:  21  loss:  2.1632444858551025\n",
      "epoch:  22  loss:  2.1628458499908447\n",
      "epoch:  23  loss:  2.1624181270599365\n",
      "epoch:  24  loss:  2.160593271255493\n",
      "epoch:  25  loss:  2.160588026046753\n",
      "epoch:  26  loss:  2.160005569458008\n",
      "epoch:  27  loss:  2.158400535583496\n",
      "epoch:  28  loss:  2.1573169231414795\n",
      "epoch:  29  loss:  2.157205581665039\n",
      "epoch:  30  loss:  2.1554834842681885\n",
      "epoch:  31  loss:  2.154832124710083\n",
      "epoch:  32  loss:  2.1543400287628174\n",
      "epoch:  33  loss:  2.1530613899230957\n",
      "epoch:  34  loss:  2.152087450027466\n",
      "epoch:  35  loss:  2.15157413482666\n",
      "epoch:  36  loss:  2.150052547454834\n",
      "epoch:  37  loss:  2.1484317779541016\n",
      "epoch:  38  loss:  2.145598888397217\n",
      "epoch:  39  loss:  2.1399002075195312\n",
      "epoch:  40  loss:  2.140532970428467\n",
      "epoch:  41  loss:  2.139343500137329\n",
      "epoch:  42  loss:  2.1361777782440186\n",
      "epoch:  43  loss:  2.1365597248077393\n",
      "epoch:  44  loss:  2.1357593536376953\n",
      "epoch:  45  loss:  2.134368658065796\n",
      "epoch:  46  loss:  2.1338253021240234\n",
      "epoch:  47  loss:  2.1324102878570557\n",
      "epoch:  48  loss:  2.1311089992523193\n",
      "epoch:  49  loss:  2.1310806274414062\n",
      "epoch:  50  loss:  2.1308164596557617\n",
      "epoch:  51  loss:  2.1295554637908936\n",
      "epoch:  52  loss:  2.128243923187256\n",
      "epoch:  53  loss:  2.1279327869415283\n",
      "epoch:  54  loss:  2.127593994140625\n",
      "epoch:  55  loss:  2.1267340183258057\n",
      "epoch:  56  loss:  2.126166820526123\n",
      "epoch:  57  loss:  2.1255581378936768\n",
      "epoch:  58  loss:  2.1247365474700928\n",
      "epoch:  59  loss:  2.124344825744629\n",
      "epoch:  60  loss:  2.1240439414978027\n",
      "epoch:  61  loss:  2.123236656188965\n",
      "epoch:  62  loss:  2.122629404067993\n",
      "epoch:  63  loss:  2.12225079536438\n",
      "epoch:  64  loss:  2.121680498123169\n",
      "epoch:  65  loss:  2.1211233139038086\n",
      "epoch:  66  loss:  2.1204817295074463\n",
      "epoch:  67  loss:  2.119368553161621\n",
      "epoch:  68  loss:  2.1176795959472656\n",
      "epoch:  69  loss:  2.114131450653076\n",
      "epoch:  70  loss:  2.107945203781128\n",
      "epoch:  71  loss:  2.110532522201538\n",
      "epoch:  72  loss:  2.108564853668213\n",
      "epoch:  73  loss:  2.1065704822540283\n",
      "epoch:  74  loss:  2.1065673828125\n",
      "epoch:  75  loss:  2.1053545475006104\n",
      "epoch:  76  loss:  2.1058266162872314\n",
      "epoch:  77  loss:  2.104340076446533\n",
      "epoch:  78  loss:  2.1022419929504395\n",
      "epoch:  79  loss:  2.102904796600342\n",
      "epoch:  80  loss:  2.103060245513916\n",
      "epoch:  81  loss:  2.1018099784851074\n",
      "epoch:  82  loss:  2.100710391998291\n",
      "epoch:  83  loss:  2.1001334190368652\n",
      "epoch:  84  loss:  2.099989414215088\n",
      "epoch:  85  loss:  2.1000165939331055\n",
      "epoch:  86  loss:  2.099320888519287\n",
      "epoch:  87  loss:  2.098222017288208\n",
      "epoch:  88  loss:  2.0979602336883545\n",
      "epoch:  89  loss:  2.097850799560547\n",
      "epoch:  90  loss:  2.0974419116973877\n",
      "epoch:  91  loss:  2.0970687866210938\n",
      "epoch:  92  loss:  2.0963633060455322\n",
      "epoch:  93  loss:  2.0960114002227783\n",
      "epoch:  94  loss:  2.095961809158325\n",
      "epoch:  95  loss:  2.0954670906066895\n",
      "epoch:  96  loss:  2.0950911045074463\n",
      "epoch:  97  loss:  2.09468936920166\n",
      "epoch:  98  loss:  2.0943171977996826\n",
      "epoch:  99  loss:  2.0942037105560303\n",
      "epoch:  100  loss:  2.0937795639038086\n",
      "epoch:  101  loss:  2.0934062004089355\n",
      "epoch:  102  loss:  2.093127489089966\n",
      "epoch:  103  loss:  2.0928187370300293\n",
      "epoch:  104  loss:  2.09261155128479\n",
      "epoch:  105  loss:  2.0922322273254395\n",
      "epoch:  106  loss:  2.091928243637085\n",
      "epoch:  107  loss:  2.0916926860809326\n",
      "epoch:  108  loss:  2.091419219970703\n",
      "epoch:  109  loss:  2.0911688804626465\n",
      "epoch:  110  loss:  2.0908498764038086\n",
      "epoch:  111  loss:  2.0906224250793457\n",
      "epoch:  112  loss:  2.090385913848877\n",
      "epoch:  113  loss:  2.090118169784546\n",
      "epoch:  114  loss:  2.089862823486328\n",
      "epoch:  115  loss:  2.089613437652588\n",
      "epoch:  116  loss:  2.089402675628662\n",
      "epoch:  117  loss:  2.089141607284546\n",
      "epoch:  118  loss:  2.0889134407043457\n",
      "epoch:  119  loss:  2.088684320449829\n",
      "epoch:  120  loss:  2.0884616374969482\n",
      "epoch:  121  loss:  2.088228702545166\n",
      "epoch:  122  loss:  2.0880045890808105\n",
      "epoch:  123  loss:  2.0877957344055176\n",
      "epoch:  124  loss:  2.0875682830810547\n",
      "epoch:  125  loss:  2.087353467941284\n",
      "epoch:  126  loss:  2.0871386528015137\n",
      "epoch:  127  loss:  2.086930751800537\n",
      "epoch:  128  loss:  2.0867040157318115\n",
      "epoch:  129  loss:  2.086498737335205\n",
      "epoch:  130  loss:  2.086287260055542\n",
      "epoch:  131  loss:  2.0860726833343506\n",
      "epoch:  132  loss:  2.085850238800049\n",
      "epoch:  133  loss:  2.085620403289795\n",
      "epoch:  134  loss:  2.08532977104187\n",
      "epoch:  135  loss:  2.08491849899292\n",
      "epoch:  136  loss:  2.0840909481048584\n",
      "epoch:  137  loss:  2.081737518310547\n",
      "epoch:  138  loss:  2.082557201385498\n",
      "epoch:  139  loss:  2.0787713527679443\n",
      "epoch:  140  loss:  2.078303575515747\n",
      "epoch:  141  loss:  2.073951244354248\n",
      "epoch:  142  loss:  2.072298049926758\n",
      "epoch:  143  loss:  2.0697617530822754\n",
      "epoch:  144  loss:  2.06756591796875\n",
      "epoch:  145  loss:  2.0663840770721436\n",
      "epoch:  146  loss:  2.0655248165130615\n",
      "epoch:  147  loss:  2.0627431869506836\n",
      "epoch:  148  loss:  2.063579559326172\n",
      "epoch:  149  loss:  2.0612196922302246\n",
      "epoch:  150  loss:  2.0610382556915283\n",
      "epoch:  151  loss:  2.059906005859375\n",
      "epoch:  152  loss:  2.0596706867218018\n",
      "epoch:  153  loss:  2.058511734008789\n",
      "epoch:  154  loss:  2.0580766201019287\n",
      "epoch:  155  loss:  2.0577685832977295\n",
      "epoch:  156  loss:  2.0569207668304443\n",
      "epoch:  157  loss:  2.0565733909606934\n",
      "epoch:  158  loss:  2.0559933185577393\n",
      "epoch:  159  loss:  2.0557336807250977\n",
      "epoch:  160  loss:  2.05466890335083\n",
      "epoch:  161  loss:  2.053849458694458\n",
      "epoch:  162  loss:  2.051046133041382\n",
      "epoch:  163  loss:  2.044879198074341\n",
      "epoch:  164  loss:  2.040283679962158\n",
      "epoch:  165  loss:  2.044545888900757\n",
      "epoch:  166  loss:  2.040234088897705\n",
      "epoch:  167  loss:  2.0396077632904053\n",
      "epoch:  168  loss:  2.0400733947753906\n",
      "epoch:  169  loss:  2.0390584468841553\n",
      "epoch:  170  loss:  2.036949396133423\n",
      "epoch:  171  loss:  2.037337303161621\n",
      "epoch:  172  loss:  2.036593198776245\n",
      "epoch:  173  loss:  2.035592555999756\n",
      "epoch:  174  loss:  2.034414768218994\n",
      "epoch:  175  loss:  2.035050630569458\n",
      "epoch:  176  loss:  2.034172296524048\n",
      "epoch:  177  loss:  2.0331785678863525\n",
      "epoch:  178  loss:  2.0325679779052734\n",
      "epoch:  179  loss:  2.0327446460723877\n",
      "epoch:  180  loss:  2.0322678089141846\n",
      "epoch:  181  loss:  2.031338930130005\n",
      "epoch:  182  loss:  2.0309300422668457\n",
      "epoch:  183  loss:  2.030829668045044\n",
      "epoch:  184  loss:  2.0306396484375\n",
      "epoch:  185  loss:  2.029890775680542\n",
      "epoch:  186  loss:  2.029533863067627\n",
      "epoch:  187  loss:  2.029242515563965\n",
      "epoch:  188  loss:  2.029200553894043\n",
      "epoch:  189  loss:  2.0285816192626953\n",
      "epoch:  190  loss:  2.0282223224639893\n",
      "epoch:  191  loss:  2.027939796447754\n",
      "epoch:  192  loss:  2.0278563499450684\n",
      "epoch:  193  loss:  2.0273759365081787\n",
      "epoch:  194  loss:  2.027050018310547\n",
      "epoch:  195  loss:  2.0268006324768066\n",
      "epoch:  196  loss:  2.0266473293304443\n",
      "epoch:  197  loss:  2.026233673095703\n",
      "epoch:  198  loss:  2.025970220565796\n",
      "epoch:  199  loss:  2.025716543197632\n",
      "epoch:  200  loss:  2.025519609451294\n",
      "epoch:  201  loss:  2.025179147720337\n",
      "epoch:  202  loss:  2.0249314308166504\n",
      "epoch:  203  loss:  2.0246827602386475\n",
      "epoch:  204  loss:  2.0244741439819336\n",
      "epoch:  205  loss:  2.024162769317627\n",
      "epoch:  206  loss:  2.0239126682281494\n",
      "epoch:  207  loss:  2.023690938949585\n",
      "epoch:  208  loss:  2.0234546661376953\n",
      "epoch:  209  loss:  2.0231728553771973\n",
      "epoch:  210  loss:  2.0229408740997314\n",
      "epoch:  211  loss:  2.0227227210998535\n",
      "epoch:  212  loss:  2.022453784942627\n",
      "epoch:  213  loss:  2.02221417427063\n",
      "epoch:  214  loss:  2.021974802017212\n",
      "epoch:  215  loss:  2.0217478275299072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  216  loss:  2.0214898586273193\n",
      "epoch:  217  loss:  2.021256685256958\n",
      "epoch:  218  loss:  2.021023988723755\n",
      "epoch:  219  loss:  2.0207743644714355\n",
      "epoch:  220  loss:  2.020538806915283\n",
      "epoch:  221  loss:  2.0203049182891846\n",
      "epoch:  222  loss:  2.02006459236145\n",
      "epoch:  223  loss:  2.0198240280151367\n",
      "epoch:  224  loss:  2.0195884704589844\n",
      "epoch:  225  loss:  2.019348621368408\n",
      "epoch:  226  loss:  2.01910662651062\n",
      "epoch:  227  loss:  2.018873453140259\n",
      "epoch:  228  loss:  2.0186309814453125\n",
      "epoch:  229  loss:  2.0183963775634766\n",
      "epoch:  230  loss:  2.018155336380005\n",
      "epoch:  231  loss:  2.017915725708008\n",
      "epoch:  232  loss:  2.017677068710327\n",
      "epoch:  233  loss:  2.0174384117126465\n",
      "epoch:  234  loss:  2.017197847366333\n",
      "epoch:  235  loss:  2.0169618129730225\n",
      "epoch:  236  loss:  2.016720771789551\n",
      "epoch:  237  loss:  2.016474485397339\n",
      "epoch:  238  loss:  2.0162360668182373\n",
      "epoch:  239  loss:  2.015995502471924\n",
      "epoch:  240  loss:  2.0157523155212402\n",
      "epoch:  241  loss:  2.0155134201049805\n",
      "epoch:  242  loss:  2.015268564224243\n",
      "epoch:  243  loss:  2.0150258541107178\n",
      "epoch:  244  loss:  2.014784336090088\n",
      "epoch:  245  loss:  2.014540195465088\n",
      "epoch:  246  loss:  2.014296293258667\n",
      "epoch:  247  loss:  2.014055013656616\n",
      "epoch:  248  loss:  2.013808488845825\n",
      "epoch:  249  loss:  2.013563632965088\n",
      "epoch:  250  loss:  2.0133185386657715\n",
      "epoch:  251  loss:  2.0130701065063477\n",
      "epoch:  252  loss:  2.012829065322876\n",
      "epoch:  253  loss:  2.012580394744873\n",
      "epoch:  254  loss:  2.0123374462127686\n",
      "epoch:  255  loss:  2.0120890140533447\n",
      "epoch:  256  loss:  2.0118415355682373\n",
      "epoch:  257  loss:  2.0115914344787598\n",
      "epoch:  258  loss:  2.0113470554351807\n",
      "epoch:  259  loss:  2.011098623275757\n",
      "epoch:  260  loss:  2.010847330093384\n",
      "epoch:  261  loss:  2.0105996131896973\n",
      "epoch:  262  loss:  2.0103507041931152\n",
      "epoch:  263  loss:  2.0101001262664795\n",
      "epoch:  264  loss:  2.0098531246185303\n",
      "epoch:  265  loss:  2.009601354598999\n",
      "epoch:  266  loss:  2.0093581676483154\n",
      "epoch:  267  loss:  2.0091004371643066\n",
      "epoch:  268  loss:  2.0088515281677246\n",
      "epoch:  269  loss:  2.008598566055298\n",
      "epoch:  270  loss:  2.008352279663086\n",
      "epoch:  271  loss:  2.008103132247925\n",
      "epoch:  272  loss:  2.0078506469726562\n",
      "epoch:  273  loss:  2.0076003074645996\n",
      "epoch:  274  loss:  2.0073530673980713\n",
      "epoch:  275  loss:  2.007101058959961\n",
      "epoch:  276  loss:  2.0068535804748535\n",
      "epoch:  277  loss:  2.0066030025482178\n",
      "epoch:  278  loss:  2.0063586235046387\n",
      "epoch:  279  loss:  2.0061147212982178\n",
      "epoch:  280  loss:  2.0058834552764893\n",
      "epoch:  281  loss:  2.005662679672241\n",
      "epoch:  282  loss:  2.0054657459259033\n",
      "epoch:  283  loss:  2.0052707195281982\n",
      "epoch:  284  loss:  2.0051121711730957\n",
      "epoch:  285  loss:  2.004906177520752\n",
      "epoch:  286  loss:  2.004697561264038\n",
      "epoch:  287  loss:  2.004363536834717\n",
      "epoch:  288  loss:  2.0040364265441895\n",
      "epoch:  289  loss:  2.0037150382995605\n",
      "epoch:  290  loss:  2.0034408569335938\n",
      "epoch:  291  loss:  2.003216505050659\n",
      "epoch:  292  loss:  2.003034830093384\n",
      "epoch:  293  loss:  2.0028738975524902\n",
      "epoch:  294  loss:  2.0027096271514893\n",
      "epoch:  295  loss:  2.0025625228881836\n",
      "epoch:  296  loss:  2.002333879470825\n",
      "epoch:  297  loss:  2.0021042823791504\n",
      "epoch:  298  loss:  2.001793622970581\n",
      "epoch:  299  loss:  2.0015013217926025\n",
      "epoch:  300  loss:  2.001209259033203\n",
      "epoch:  301  loss:  2.0009543895721436\n",
      "epoch:  302  loss:  2.000732421875\n",
      "epoch:  303  loss:  2.000535726547241\n",
      "epoch:  304  loss:  2.0003483295440674\n",
      "epoch:  305  loss:  2.0001800060272217\n",
      "epoch:  306  loss:  2.000051736831665\n",
      "epoch:  307  loss:  1.999914288520813\n",
      "epoch:  308  loss:  1.9998453855514526\n",
      "epoch:  309  loss:  1.9996615648269653\n",
      "epoch:  310  loss:  1.9995262622833252\n",
      "epoch:  311  loss:  1.9991666078567505\n",
      "epoch:  312  loss:  1.9988234043121338\n",
      "epoch:  313  loss:  1.9984668493270874\n",
      "epoch:  314  loss:  1.9981893301010132\n",
      "epoch:  315  loss:  1.9979963302612305\n",
      "epoch:  316  loss:  1.9978575706481934\n",
      "epoch:  317  loss:  1.997766375541687\n",
      "epoch:  318  loss:  1.9976413249969482\n",
      "epoch:  319  loss:  1.9975383281707764\n",
      "epoch:  320  loss:  1.9973043203353882\n",
      "epoch:  321  loss:  1.9970743656158447\n",
      "epoch:  322  loss:  1.996744990348816\n",
      "epoch:  323  loss:  1.9964507818222046\n",
      "epoch:  324  loss:  1.9961892366409302\n",
      "epoch:  325  loss:  1.995972990989685\n",
      "epoch:  326  loss:  1.9958009719848633\n",
      "epoch:  327  loss:  1.9956483840942383\n",
      "epoch:  328  loss:  1.9955273866653442\n",
      "epoch:  329  loss:  1.9954081773757935\n",
      "epoch:  330  loss:  1.9953444004058838\n",
      "epoch:  331  loss:  1.9951895475387573\n",
      "epoch:  332  loss:  1.9950981140136719\n",
      "epoch:  333  loss:  1.9948163032531738\n",
      "epoch:  334  loss:  1.994573712348938\n",
      "epoch:  335  loss:  1.9942221641540527\n",
      "epoch:  336  loss:  1.9939316511154175\n",
      "epoch:  337  loss:  1.9936681985855103\n",
      "epoch:  338  loss:  1.9934613704681396\n",
      "epoch:  339  loss:  1.9933011531829834\n",
      "epoch:  340  loss:  1.9931588172912598\n",
      "epoch:  341  loss:  1.9930635690689087\n",
      "epoch:  342  loss:  1.9929648637771606\n",
      "epoch:  343  loss:  1.9929356575012207\n",
      "epoch:  344  loss:  1.9928100109100342\n",
      "epoch:  345  loss:  1.9927541017532349\n",
      "epoch:  346  loss:  1.992442011833191\n",
      "epoch:  347  loss:  1.9921824932098389\n",
      "epoch:  348  loss:  1.9917914867401123\n",
      "epoch:  349  loss:  1.9914840459823608\n",
      "epoch:  350  loss:  1.9912378787994385\n",
      "epoch:  351  loss:  1.9910622835159302\n",
      "epoch:  352  loss:  1.990950107574463\n",
      "epoch:  353  loss:  1.9908570051193237\n",
      "epoch:  354  loss:  1.9908301830291748\n",
      "epoch:  355  loss:  1.990728735923767\n",
      "epoch:  356  loss:  1.9906895160675049\n",
      "epoch:  357  loss:  1.9904361963272095\n",
      "epoch:  358  loss:  1.9902217388153076\n",
      "epoch:  359  loss:  1.9898583889007568\n",
      "epoch:  360  loss:  1.9895679950714111\n",
      "epoch:  361  loss:  1.9892982244491577\n",
      "epoch:  362  loss:  1.9890958070755005\n",
      "epoch:  363  loss:  1.9889392852783203\n",
      "epoch:  364  loss:  1.98881196975708\n",
      "epoch:  365  loss:  1.9887303113937378\n",
      "epoch:  366  loss:  1.9886445999145508\n",
      "epoch:  367  loss:  1.9886348247528076\n",
      "epoch:  368  loss:  1.9885166883468628\n",
      "epoch:  369  loss:  1.9884934425354004\n",
      "epoch:  370  loss:  1.9882025718688965\n",
      "epoch:  371  loss:  1.9879757165908813\n",
      "epoch:  372  loss:  1.9875946044921875\n",
      "epoch:  373  loss:  1.987283706665039\n",
      "epoch:  374  loss:  1.9870182275772095\n",
      "epoch:  375  loss:  1.986807942390442\n",
      "epoch:  376  loss:  1.9866482019424438\n",
      "epoch:  377  loss:  1.9865070581436157\n",
      "epoch:  378  loss:  1.9863637685775757\n",
      "epoch:  379  loss:  1.986088752746582\n",
      "epoch:  380  loss:  1.985541820526123\n",
      "epoch:  381  loss:  1.98389732837677\n",
      "epoch:  382  loss:  1.9807416200637817\n",
      "epoch:  383  loss:  1.9712507724761963\n",
      "epoch:  384  loss:  1.968383550643921\n",
      "epoch:  385  loss:  1.959174394607544\n",
      "epoch:  386  loss:  1.9546751976013184\n",
      "epoch:  387  loss:  1.951723337173462\n",
      "epoch:  388  loss:  1.9471498727798462\n",
      "epoch:  389  loss:  1.9463664293289185\n",
      "epoch:  390  loss:  1.9452861547470093\n",
      "epoch:  391  loss:  1.943949818611145\n",
      "epoch:  392  loss:  1.942449688911438\n",
      "epoch:  393  loss:  1.941618800163269\n",
      "epoch:  394  loss:  1.9409387111663818\n",
      "epoch:  395  loss:  1.939624309539795\n",
      "epoch:  396  loss:  1.939078688621521\n",
      "epoch:  397  loss:  1.9380282163619995\n",
      "epoch:  398  loss:  1.9378085136413574\n",
      "epoch:  399  loss:  1.9372459650039673\n",
      "epoch:  400  loss:  1.936753749847412\n",
      "epoch:  401  loss:  1.9361422061920166\n",
      "epoch:  402  loss:  1.9355037212371826\n",
      "epoch:  403  loss:  1.9351832866668701\n",
      "epoch:  404  loss:  1.9346719980239868\n",
      "epoch:  405  loss:  1.9343358278274536\n",
      "epoch:  406  loss:  1.9337308406829834\n",
      "epoch:  407  loss:  1.9333415031433105\n",
      "epoch:  408  loss:  1.9328888654708862\n",
      "epoch:  409  loss:  1.9324806928634644\n",
      "epoch:  410  loss:  1.932132601737976\n",
      "epoch:  411  loss:  1.9316208362579346\n",
      "epoch:  412  loss:  1.931276559829712\n",
      "epoch:  413  loss:  1.930850625038147\n",
      "epoch:  414  loss:  1.9305665493011475\n",
      "epoch:  415  loss:  1.93027925491333\n",
      "epoch:  416  loss:  1.9301693439483643\n",
      "epoch:  417  loss:  1.9302293062210083\n",
      "epoch:  418  loss:  1.930668830871582\n",
      "epoch:  419  loss:  1.9302880764007568\n",
      "epoch:  420  loss:  1.9303113222122192\n",
      "epoch:  421  loss:  1.9284464120864868\n",
      "epoch:  422  loss:  1.9261142015457153\n",
      "epoch:  423  loss:  1.9211158752441406\n",
      "epoch:  424  loss:  1.9129633903503418\n",
      "epoch:  425  loss:  1.9150826930999756\n",
      "epoch:  426  loss:  1.9087682962417603\n",
      "epoch:  427  loss:  1.9034405946731567\n",
      "epoch:  428  loss:  1.9024238586425781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  429  loss:  1.9004926681518555\n",
      "epoch:  430  loss:  1.8991914987564087\n",
      "epoch:  431  loss:  1.8979350328445435\n",
      "epoch:  432  loss:  1.8962249755859375\n",
      "epoch:  433  loss:  1.8979884386062622\n",
      "epoch:  434  loss:  1.895485520362854\n",
      "epoch:  435  loss:  1.8933027982711792\n",
      "epoch:  436  loss:  1.8925955295562744\n",
      "epoch:  437  loss:  1.893183708190918\n",
      "epoch:  438  loss:  1.8927505016326904\n",
      "epoch:  439  loss:  1.8905471563339233\n",
      "epoch:  440  loss:  1.8895410299301147\n",
      "epoch:  441  loss:  1.890012502670288\n",
      "epoch:  442  loss:  1.889082670211792\n",
      "epoch:  443  loss:  1.887732744216919\n",
      "epoch:  444  loss:  1.887952446937561\n",
      "epoch:  445  loss:  1.8873064517974854\n",
      "epoch:  446  loss:  1.8866162300109863\n",
      "epoch:  447  loss:  1.8863874673843384\n",
      "epoch:  448  loss:  1.8857078552246094\n",
      "epoch:  449  loss:  1.885438323020935\n",
      "epoch:  450  loss:  1.8848899602890015\n",
      "epoch:  451  loss:  1.884441614151001\n",
      "epoch:  452  loss:  1.884110689163208\n",
      "epoch:  453  loss:  1.8836166858673096\n",
      "epoch:  454  loss:  1.883188009262085\n",
      "epoch:  455  loss:  1.8827142715454102\n",
      "epoch:  456  loss:  1.8824622631072998\n",
      "epoch:  457  loss:  1.8818895816802979\n",
      "epoch:  458  loss:  1.881467342376709\n",
      "epoch:  459  loss:  1.8810702562332153\n",
      "epoch:  460  loss:  1.8806118965148926\n",
      "epoch:  461  loss:  1.8803434371948242\n",
      "epoch:  462  loss:  1.879800796508789\n",
      "epoch:  463  loss:  1.8794103860855103\n",
      "epoch:  464  loss:  1.87898588180542\n",
      "epoch:  465  loss:  1.8785643577575684\n",
      "epoch:  466  loss:  1.8782597780227661\n",
      "epoch:  467  loss:  1.877792477607727\n",
      "epoch:  468  loss:  1.8774107694625854\n",
      "epoch:  469  loss:  1.8769663572311401\n",
      "epoch:  470  loss:  1.8765556812286377\n",
      "epoch:  471  loss:  1.8762023448944092\n",
      "epoch:  472  loss:  1.875794768333435\n",
      "epoch:  473  loss:  1.87542724609375\n",
      "epoch:  474  loss:  1.8750123977661133\n",
      "epoch:  475  loss:  1.8746072053909302\n",
      "epoch:  476  loss:  1.8742042779922485\n",
      "epoch:  477  loss:  1.8738137483596802\n",
      "epoch:  478  loss:  1.8734241724014282\n",
      "epoch:  479  loss:  1.8730485439300537\n",
      "epoch:  480  loss:  1.872665524482727\n",
      "epoch:  481  loss:  1.8722889423370361\n",
      "epoch:  482  loss:  1.8719165325164795\n",
      "epoch:  483  loss:  1.8715354204177856\n",
      "epoch:  484  loss:  1.8711806535720825\n",
      "epoch:  485  loss:  1.8708124160766602\n",
      "epoch:  486  loss:  1.8704787492752075\n",
      "epoch:  487  loss:  1.8701587915420532\n",
      "epoch:  488  loss:  1.8698886632919312\n",
      "epoch:  489  loss:  1.8697298765182495\n",
      "epoch:  490  loss:  1.8696316480636597\n",
      "epoch:  491  loss:  1.8697938919067383\n",
      "epoch:  492  loss:  1.8697423934936523\n",
      "epoch:  493  loss:  1.8698511123657227\n",
      "epoch:  494  loss:  1.8688790798187256\n",
      "epoch:  495  loss:  1.8678313493728638\n",
      "epoch:  496  loss:  1.866819977760315\n",
      "epoch:  497  loss:  1.866288423538208\n",
      "epoch:  498  loss:  1.8661906719207764\n",
      "epoch:  499  loss:  1.86625337600708\n",
      "epoch:  500  loss:  1.8663939237594604\n",
      "epoch:  501  loss:  1.8660054206848145\n",
      "epoch:  502  loss:  1.8654645681381226\n",
      "epoch:  503  loss:  1.8645603656768799\n",
      "epoch:  504  loss:  1.863865613937378\n",
      "epoch:  505  loss:  1.8634800910949707\n",
      "epoch:  506  loss:  1.863342523574829\n",
      "epoch:  507  loss:  1.8633205890655518\n",
      "epoch:  508  loss:  1.863132119178772\n",
      "epoch:  509  loss:  1.8628350496292114\n",
      "epoch:  510  loss:  1.8622310161590576\n",
      "epoch:  511  loss:  1.8616304397583008\n",
      "epoch:  512  loss:  1.8610742092132568\n",
      "epoch:  513  loss:  1.8606719970703125\n",
      "epoch:  514  loss:  1.860400915145874\n",
      "epoch:  515  loss:  1.8602029085159302\n",
      "epoch:  516  loss:  1.8600382804870605\n",
      "epoch:  517  loss:  1.8597853183746338\n",
      "epoch:  518  loss:  1.8595114946365356\n",
      "epoch:  519  loss:  1.8590924739837646\n",
      "epoch:  520  loss:  1.8586516380310059\n",
      "epoch:  521  loss:  1.8581490516662598\n",
      "epoch:  522  loss:  1.8576810359954834\n",
      "epoch:  523  loss:  1.8572484254837036\n",
      "epoch:  524  loss:  1.8568737506866455\n",
      "epoch:  525  loss:  1.8565397262573242\n",
      "epoch:  526  loss:  1.8562326431274414\n",
      "epoch:  527  loss:  1.8559612035751343\n",
      "epoch:  528  loss:  1.8557120561599731\n",
      "epoch:  529  loss:  1.8555198907852173\n",
      "epoch:  530  loss:  1.855356216430664\n",
      "epoch:  531  loss:  1.8552874326705933\n",
      "epoch:  532  loss:  1.8551621437072754\n",
      "epoch:  533  loss:  1.8550794124603271\n",
      "epoch:  534  loss:  1.8546169996261597\n",
      "epoch:  535  loss:  1.8540760278701782\n",
      "epoch:  536  loss:  1.8532925844192505\n",
      "epoch:  537  loss:  1.8526394367218018\n",
      "epoch:  538  loss:  1.852107286453247\n",
      "epoch:  539  loss:  1.8517320156097412\n",
      "epoch:  540  loss:  1.851494550704956\n",
      "epoch:  541  loss:  1.8513340950012207\n",
      "epoch:  542  loss:  1.8512625694274902\n",
      "epoch:  543  loss:  1.8511486053466797\n",
      "epoch:  544  loss:  1.8510642051696777\n",
      "epoch:  545  loss:  1.8507097959518433\n",
      "epoch:  546  loss:  1.8502835035324097\n",
      "epoch:  547  loss:  1.8495882749557495\n",
      "epoch:  548  loss:  1.8489532470703125\n",
      "epoch:  549  loss:  1.8483930826187134\n",
      "epoch:  550  loss:  1.847968578338623\n",
      "epoch:  551  loss:  1.8476598262786865\n",
      "epoch:  552  loss:  1.8474299907684326\n",
      "epoch:  553  loss:  1.8472768068313599\n",
      "epoch:  554  loss:  1.8471559286117554\n",
      "epoch:  555  loss:  1.8471012115478516\n",
      "epoch:  556  loss:  1.8469523191452026\n",
      "epoch:  557  loss:  1.8467367887496948\n",
      "epoch:  558  loss:  1.8462859392166138\n",
      "epoch:  559  loss:  1.8456391096115112\n",
      "epoch:  560  loss:  1.8449667692184448\n",
      "epoch:  561  loss:  1.8443740606307983\n",
      "epoch:  562  loss:  1.843956708908081\n",
      "epoch:  563  loss:  1.8436644077301025\n",
      "epoch:  564  loss:  1.8434945344924927\n",
      "epoch:  565  loss:  1.8433763980865479\n",
      "epoch:  566  loss:  1.8433525562286377\n",
      "epoch:  567  loss:  1.843174695968628\n",
      "epoch:  568  loss:  1.8430068492889404\n",
      "epoch:  569  loss:  1.8424782752990723\n",
      "epoch:  570  loss:  1.84195077419281\n",
      "epoch:  571  loss:  1.841295599937439\n",
      "epoch:  572  loss:  1.8407340049743652\n",
      "epoch:  573  loss:  1.8403009176254272\n",
      "epoch:  574  loss:  1.8399989604949951\n",
      "epoch:  575  loss:  1.839827299118042\n",
      "epoch:  576  loss:  1.8397817611694336\n",
      "epoch:  577  loss:  1.8399159908294678\n",
      "epoch:  578  loss:  1.8398264646530151\n",
      "epoch:  579  loss:  1.8395580053329468\n",
      "epoch:  580  loss:  1.8385480642318726\n",
      "epoch:  581  loss:  1.8377188444137573\n",
      "epoch:  582  loss:  1.8373775482177734\n",
      "epoch:  583  loss:  1.8374297618865967\n",
      "epoch:  584  loss:  1.83755624294281\n",
      "epoch:  585  loss:  1.8371435403823853\n",
      "epoch:  586  loss:  1.8364720344543457\n",
      "epoch:  587  loss:  1.8358477354049683\n",
      "epoch:  588  loss:  1.835595726966858\n",
      "epoch:  589  loss:  1.8356707096099854\n",
      "epoch:  590  loss:  1.8354713916778564\n",
      "epoch:  591  loss:  1.835185170173645\n",
      "epoch:  592  loss:  1.8346004486083984\n",
      "epoch:  593  loss:  1.834404706954956\n",
      "epoch:  594  loss:  1.8345823287963867\n",
      "epoch:  595  loss:  1.8344794511795044\n",
      "epoch:  596  loss:  1.8342682123184204\n",
      "epoch:  597  loss:  1.8336621522903442\n",
      "epoch:  598  loss:  1.8332587480545044\n",
      "epoch:  599  loss:  1.8330248594284058\n",
      "epoch:  600  loss:  1.8322020769119263\n",
      "epoch:  601  loss:  1.8314566612243652\n",
      "epoch:  602  loss:  1.830981731414795\n",
      "epoch:  603  loss:  1.8308275938034058\n",
      "epoch:  604  loss:  1.8307924270629883\n",
      "epoch:  605  loss:  1.830557942390442\n",
      "epoch:  606  loss:  1.8303501605987549\n",
      "epoch:  607  loss:  1.8301520347595215\n",
      "epoch:  608  loss:  1.8300089836120605\n",
      "epoch:  609  loss:  1.8298718929290771\n",
      "epoch:  610  loss:  1.8292832374572754\n",
      "epoch:  611  loss:  1.8286954164505005\n",
      "epoch:  612  loss:  1.828139066696167\n",
      "epoch:  613  loss:  1.8277459144592285\n",
      "epoch:  614  loss:  1.82744300365448\n",
      "epoch:  615  loss:  1.8270736932754517\n",
      "epoch:  616  loss:  1.8266997337341309\n",
      "epoch:  617  loss:  1.826343297958374\n",
      "epoch:  618  loss:  1.826088786125183\n",
      "epoch:  619  loss:  1.825933575630188\n",
      "epoch:  620  loss:  1.82579505443573\n",
      "epoch:  621  loss:  1.8257330656051636\n",
      "epoch:  622  loss:  1.8255194425582886\n",
      "epoch:  623  loss:  1.8253904581069946\n",
      "epoch:  624  loss:  1.8252332210540771\n",
      "epoch:  625  loss:  1.8249614238739014\n",
      "epoch:  626  loss:  1.824678659439087\n",
      "epoch:  627  loss:  1.8240220546722412\n",
      "epoch:  628  loss:  1.8233916759490967\n",
      "epoch:  629  loss:  1.82276451587677\n",
      "epoch:  630  loss:  1.8223168849945068\n",
      "epoch:  631  loss:  1.8220127820968628\n",
      "epoch:  632  loss:  1.821758508682251\n",
      "epoch:  633  loss:  1.821523666381836\n",
      "epoch:  634  loss:  1.8212451934814453\n",
      "epoch:  635  loss:  1.820986270904541\n",
      "epoch:  636  loss:  1.8207401037216187\n",
      "epoch:  637  loss:  1.8206017017364502\n",
      "epoch:  638  loss:  1.8205918073654175\n",
      "epoch:  639  loss:  1.8205633163452148\n",
      "epoch:  640  loss:  1.8206641674041748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  641  loss:  1.8200483322143555\n",
      "epoch:  642  loss:  1.8193798065185547\n",
      "epoch:  643  loss:  1.8187612295150757\n",
      "epoch:  644  loss:  1.8182543516159058\n",
      "epoch:  645  loss:  1.8179162740707397\n",
      "epoch:  646  loss:  1.8175729513168335\n",
      "epoch:  647  loss:  1.8172913789749146\n",
      "epoch:  648  loss:  1.8169550895690918\n",
      "epoch:  649  loss:  1.8167387247085571\n",
      "epoch:  650  loss:  1.8165844678878784\n",
      "epoch:  651  loss:  1.8165454864501953\n",
      "epoch:  652  loss:  1.8166381120681763\n",
      "epoch:  653  loss:  1.8164399862289429\n",
      "epoch:  654  loss:  1.81620192527771\n",
      "epoch:  655  loss:  1.815547227859497\n",
      "epoch:  656  loss:  1.814903974533081\n",
      "epoch:  657  loss:  1.8143013715744019\n",
      "epoch:  658  loss:  1.8138360977172852\n",
      "epoch:  659  loss:  1.8135465383529663\n",
      "epoch:  660  loss:  1.8132963180541992\n",
      "epoch:  661  loss:  1.8130933046340942\n",
      "epoch:  662  loss:  1.8128070831298828\n",
      "epoch:  663  loss:  1.8125303983688354\n",
      "epoch:  664  loss:  1.8122327327728271\n",
      "epoch:  665  loss:  1.8120161294937134\n",
      "epoch:  666  loss:  1.8118987083435059\n",
      "epoch:  667  loss:  1.8117690086364746\n",
      "epoch:  668  loss:  1.8118067979812622\n",
      "epoch:  669  loss:  1.8113141059875488\n",
      "epoch:  670  loss:  1.8108538389205933\n",
      "epoch:  671  loss:  1.810172200202942\n",
      "epoch:  672  loss:  1.8096730709075928\n",
      "epoch:  673  loss:  1.809321641921997\n",
      "epoch:  674  loss:  1.8090695142745972\n",
      "epoch:  675  loss:  1.8088693618774414\n",
      "epoch:  676  loss:  1.8085885047912598\n",
      "epoch:  677  loss:  1.8083250522613525\n",
      "epoch:  678  loss:  1.8079242706298828\n",
      "epoch:  679  loss:  1.8075828552246094\n",
      "epoch:  680  loss:  1.8072618246078491\n",
      "epoch:  681  loss:  1.8070756196975708\n",
      "epoch:  682  loss:  1.8070347309112549\n",
      "epoch:  683  loss:  1.8070529699325562\n",
      "epoch:  684  loss:  1.8072272539138794\n",
      "epoch:  685  loss:  1.807150959968567\n",
      "epoch:  686  loss:  1.8070902824401855\n",
      "epoch:  687  loss:  1.8065303564071655\n",
      "epoch:  688  loss:  1.8058435916900635\n",
      "epoch:  689  loss:  1.8052252531051636\n",
      "epoch:  690  loss:  1.8045158386230469\n",
      "epoch:  691  loss:  1.8041093349456787\n",
      "epoch:  692  loss:  1.8037850856781006\n",
      "epoch:  693  loss:  1.8036231994628906\n",
      "epoch:  694  loss:  1.8035434484481812\n",
      "epoch:  695  loss:  1.8035894632339478\n",
      "epoch:  696  loss:  1.8036412000656128\n",
      "epoch:  697  loss:  1.8033599853515625\n",
      "epoch:  698  loss:  1.8029638528823853\n",
      "epoch:  699  loss:  1.802034616470337\n",
      "epoch:  700  loss:  1.8013161420822144\n",
      "epoch:  701  loss:  1.8008793592453003\n",
      "epoch:  702  loss:  1.8007124662399292\n",
      "epoch:  703  loss:  1.800734519958496\n",
      "epoch:  704  loss:  1.800732135772705\n",
      "epoch:  705  loss:  1.8007712364196777\n",
      "epoch:  706  loss:  1.8003687858581543\n",
      "epoch:  707  loss:  1.7998945713043213\n",
      "epoch:  708  loss:  1.799331545829773\n",
      "epoch:  709  loss:  1.7988409996032715\n",
      "epoch:  710  loss:  1.7985106706619263\n",
      "epoch:  711  loss:  1.798264503479004\n",
      "epoch:  712  loss:  1.7981276512145996\n",
      "epoch:  713  loss:  1.7977858781814575\n",
      "epoch:  714  loss:  1.7975164651870728\n",
      "epoch:  715  loss:  1.797070026397705\n",
      "epoch:  716  loss:  1.796751856803894\n",
      "epoch:  717  loss:  1.7965044975280762\n",
      "epoch:  718  loss:  1.7963298559188843\n",
      "epoch:  719  loss:  1.7962342500686646\n",
      "epoch:  720  loss:  1.7960455417633057\n",
      "epoch:  721  loss:  1.7958508729934692\n",
      "epoch:  722  loss:  1.7954087257385254\n",
      "epoch:  723  loss:  1.7949271202087402\n",
      "epoch:  724  loss:  1.7944185733795166\n",
      "epoch:  725  loss:  1.7940415143966675\n",
      "epoch:  726  loss:  1.7937836647033691\n",
      "epoch:  727  loss:  1.7935912609100342\n",
      "epoch:  728  loss:  1.7935154438018799\n",
      "epoch:  729  loss:  1.7932151556015015\n",
      "epoch:  730  loss:  1.7930055856704712\n",
      "epoch:  731  loss:  1.792520523071289\n",
      "epoch:  732  loss:  1.7921652793884277\n",
      "epoch:  733  loss:  1.791860818862915\n",
      "epoch:  734  loss:  1.7916933298110962\n",
      "epoch:  735  loss:  1.7916635274887085\n",
      "epoch:  736  loss:  1.7916826009750366\n",
      "epoch:  737  loss:  1.7917723655700684\n",
      "epoch:  738  loss:  1.791671872138977\n",
      "epoch:  739  loss:  1.7915278673171997\n",
      "epoch:  740  loss:  1.7912005186080933\n",
      "epoch:  741  loss:  1.7906357049942017\n",
      "epoch:  742  loss:  1.790213942527771\n",
      "epoch:  743  loss:  1.789311170578003\n",
      "epoch:  744  loss:  1.7887362241744995\n",
      "epoch:  745  loss:  1.7883249521255493\n",
      "epoch:  746  loss:  1.7881684303283691\n",
      "epoch:  747  loss:  1.7882088422775269\n",
      "epoch:  748  loss:  1.7883532047271729\n",
      "epoch:  749  loss:  1.788644552230835\n",
      "epoch:  750  loss:  1.7882835865020752\n",
      "epoch:  751  loss:  1.78782057762146\n",
      "epoch:  752  loss:  1.7869343757629395\n",
      "epoch:  753  loss:  1.7862368822097778\n",
      "epoch:  754  loss:  1.785827875137329\n",
      "epoch:  755  loss:  1.785662055015564\n",
      "epoch:  756  loss:  1.7857139110565186\n",
      "epoch:  757  loss:  1.7855393886566162\n",
      "epoch:  758  loss:  1.7854726314544678\n",
      "epoch:  759  loss:  1.7850449085235596\n",
      "epoch:  760  loss:  1.7846845388412476\n",
      "epoch:  761  loss:  1.7843436002731323\n",
      "epoch:  762  loss:  1.7840018272399902\n",
      "epoch:  763  loss:  1.7837374210357666\n",
      "epoch:  764  loss:  1.7834049463272095\n",
      "epoch:  765  loss:  1.7831109762191772\n",
      "epoch:  766  loss:  1.78264582157135\n",
      "epoch:  767  loss:  1.7822836637496948\n",
      "epoch:  768  loss:  1.7819758653640747\n",
      "epoch:  769  loss:  1.7817738056182861\n",
      "epoch:  770  loss:  1.7816658020019531\n",
      "epoch:  771  loss:  1.7815452814102173\n",
      "epoch:  772  loss:  1.7814408540725708\n",
      "epoch:  773  loss:  1.7811866998672485\n",
      "epoch:  774  loss:  1.7808598279953003\n",
      "epoch:  775  loss:  1.780421495437622\n",
      "epoch:  776  loss:  1.7799931764602661\n",
      "epoch:  777  loss:  1.7796374559402466\n",
      "epoch:  778  loss:  1.779354214668274\n",
      "epoch:  779  loss:  1.7792456150054932\n",
      "epoch:  780  loss:  1.7789560556411743\n",
      "epoch:  781  loss:  1.7788488864898682\n",
      "epoch:  782  loss:  1.7783507108688354\n",
      "epoch:  783  loss:  1.7779828310012817\n",
      "epoch:  784  loss:  1.7775229215621948\n",
      "epoch:  785  loss:  1.7771790027618408\n",
      "epoch:  786  loss:  1.7768938541412354\n",
      "epoch:  787  loss:  1.7766551971435547\n",
      "epoch:  788  loss:  1.776465892791748\n",
      "epoch:  789  loss:  1.7762935161590576\n",
      "epoch:  790  loss:  1.7761443853378296\n",
      "epoch:  791  loss:  1.7759793996810913\n",
      "epoch:  792  loss:  1.7758337259292603\n",
      "epoch:  793  loss:  1.7756071090698242\n",
      "epoch:  794  loss:  1.7754580974578857\n",
      "epoch:  795  loss:  1.7752957344055176\n",
      "epoch:  796  loss:  1.7752645015716553\n",
      "epoch:  797  loss:  1.775566816329956\n",
      "epoch:  798  loss:  1.775067925453186\n",
      "epoch:  799  loss:  1.7748416662216187\n",
      "epoch:  800  loss:  1.7735933065414429\n",
      "epoch:  801  loss:  1.7728179693222046\n",
      "epoch:  802  loss:  1.7724196910858154\n",
      "epoch:  803  loss:  1.7723361253738403\n",
      "epoch:  804  loss:  1.7725706100463867\n",
      "epoch:  805  loss:  1.7724566459655762\n",
      "epoch:  806  loss:  1.7727006673812866\n",
      "epoch:  807  loss:  1.7720633745193481\n",
      "epoch:  808  loss:  1.7717052698135376\n",
      "epoch:  809  loss:  1.7713305950164795\n",
      "epoch:  810  loss:  1.7709851264953613\n",
      "epoch:  811  loss:  1.7707794904708862\n",
      "epoch:  812  loss:  1.7703429460525513\n",
      "epoch:  813  loss:  1.770015835762024\n",
      "epoch:  814  loss:  1.7693861722946167\n",
      "epoch:  815  loss:  1.7690383195877075\n",
      "epoch:  816  loss:  1.7688833475112915\n",
      "epoch:  817  loss:  1.76889967918396\n",
      "epoch:  818  loss:  1.768979549407959\n",
      "epoch:  819  loss:  1.7687320709228516\n",
      "epoch:  820  loss:  1.7683402299880981\n",
      "epoch:  821  loss:  1.7676661014556885\n",
      "epoch:  822  loss:  1.7670763731002808\n",
      "epoch:  823  loss:  1.7666720151901245\n",
      "epoch:  824  loss:  1.766453742980957\n",
      "epoch:  825  loss:  1.766363263130188\n",
      "epoch:  826  loss:  1.7662287950515747\n",
      "epoch:  827  loss:  1.7660503387451172\n",
      "epoch:  828  loss:  1.7656691074371338\n",
      "epoch:  829  loss:  1.765270709991455\n",
      "epoch:  830  loss:  1.7648438215255737\n",
      "epoch:  831  loss:  1.7645111083984375\n",
      "epoch:  832  loss:  1.764297366142273\n",
      "epoch:  833  loss:  1.7641469240188599\n",
      "epoch:  834  loss:  1.7641772031784058\n",
      "epoch:  835  loss:  1.7639254331588745\n",
      "epoch:  836  loss:  1.7639483213424683\n",
      "epoch:  837  loss:  1.763405442237854\n",
      "epoch:  838  loss:  1.7631927728652954\n",
      "epoch:  839  loss:  1.7629289627075195\n",
      "epoch:  840  loss:  1.7628650665283203\n",
      "epoch:  841  loss:  1.7628968954086304\n",
      "epoch:  842  loss:  1.7625854015350342\n",
      "epoch:  843  loss:  1.762211561203003\n",
      "epoch:  844  loss:  1.7616448402404785\n",
      "epoch:  845  loss:  1.761124610900879\n",
      "epoch:  846  loss:  1.7607991695404053\n",
      "epoch:  847  loss:  1.7604750394821167\n",
      "epoch:  848  loss:  1.7603695392608643\n",
      "epoch:  849  loss:  1.7599430084228516\n",
      "epoch:  850  loss:  1.7596173286437988\n",
      "epoch:  851  loss:  1.7591814994812012\n",
      "epoch:  852  loss:  1.7588350772857666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  853  loss:  1.7585536241531372\n",
      "epoch:  854  loss:  1.7583470344543457\n",
      "epoch:  855  loss:  1.7582428455352783\n",
      "epoch:  856  loss:  1.7581157684326172\n",
      "epoch:  857  loss:  1.7582075595855713\n",
      "epoch:  858  loss:  1.757878065109253\n",
      "epoch:  859  loss:  1.7578171491622925\n",
      "epoch:  860  loss:  1.757344126701355\n",
      "epoch:  861  loss:  1.7571004629135132\n",
      "epoch:  862  loss:  1.7569248676300049\n",
      "epoch:  863  loss:  1.7566639184951782\n",
      "epoch:  864  loss:  1.7564842700958252\n",
      "epoch:  865  loss:  1.755971908569336\n",
      "epoch:  866  loss:  1.7554975748062134\n",
      "epoch:  867  loss:  1.754995346069336\n",
      "epoch:  868  loss:  1.7545708417892456\n",
      "epoch:  869  loss:  1.754249930381775\n",
      "epoch:  870  loss:  1.7540241479873657\n",
      "epoch:  871  loss:  1.7538537979125977\n",
      "epoch:  872  loss:  1.753692388534546\n",
      "epoch:  873  loss:  1.753547191619873\n",
      "epoch:  874  loss:  1.7533327341079712\n",
      "epoch:  875  loss:  1.7531607151031494\n",
      "epoch:  876  loss:  1.752809762954712\n",
      "epoch:  877  loss:  1.7525330781936646\n",
      "epoch:  878  loss:  1.752214789390564\n",
      "epoch:  879  loss:  1.7520185708999634\n",
      "epoch:  880  loss:  1.7520170211791992\n",
      "epoch:  881  loss:  1.7519255876541138\n",
      "epoch:  882  loss:  1.7522947788238525\n",
      "epoch:  883  loss:  1.75154447555542\n",
      "epoch:  884  loss:  1.7511839866638184\n",
      "epoch:  885  loss:  1.750510334968567\n",
      "epoch:  886  loss:  1.7500978708267212\n",
      "epoch:  887  loss:  1.7498506307601929\n",
      "epoch:  888  loss:  1.7496317625045776\n",
      "epoch:  889  loss:  1.7494992017745972\n",
      "epoch:  890  loss:  1.7491546869277954\n",
      "epoch:  891  loss:  1.7489184141159058\n",
      "epoch:  892  loss:  1.748467206954956\n",
      "epoch:  893  loss:  1.7481927871704102\n",
      "epoch:  894  loss:  1.7479668855667114\n",
      "epoch:  895  loss:  1.7478927373886108\n",
      "epoch:  896  loss:  1.7479370832443237\n",
      "epoch:  897  loss:  1.7478843927383423\n",
      "epoch:  898  loss:  1.7478513717651367\n",
      "epoch:  899  loss:  1.7474218606948853\n",
      "epoch:  900  loss:  1.746977686882019\n",
      "epoch:  901  loss:  1.7464903593063354\n",
      "epoch:  902  loss:  1.7460310459136963\n",
      "epoch:  903  loss:  1.7458051443099976\n",
      "epoch:  904  loss:  1.7454428672790527\n",
      "epoch:  905  loss:  1.745255947113037\n",
      "epoch:  906  loss:  1.744869351387024\n",
      "epoch:  907  loss:  1.7445552349090576\n",
      "epoch:  908  loss:  1.7442355155944824\n",
      "epoch:  909  loss:  1.743994951248169\n",
      "epoch:  910  loss:  1.7438347339630127\n",
      "epoch:  911  loss:  1.743718147277832\n",
      "epoch:  912  loss:  1.74378502368927\n",
      "epoch:  913  loss:  1.7434955835342407\n",
      "epoch:  914  loss:  1.7434369325637817\n",
      "epoch:  915  loss:  1.7428585290908813\n",
      "epoch:  916  loss:  1.742464303970337\n",
      "epoch:  917  loss:  1.7420448064804077\n",
      "epoch:  918  loss:  1.7417223453521729\n",
      "epoch:  919  loss:  1.7414859533309937\n",
      "epoch:  920  loss:  1.7412514686584473\n",
      "epoch:  921  loss:  1.7410593032836914\n",
      "epoch:  922  loss:  1.7407535314559937\n",
      "epoch:  923  loss:  1.7405000925064087\n",
      "epoch:  924  loss:  1.7401671409606934\n",
      "epoch:  925  loss:  1.7399137020111084\n",
      "epoch:  926  loss:  1.7396575212478638\n",
      "epoch:  927  loss:  1.7394827604293823\n",
      "epoch:  928  loss:  1.7393513917922974\n",
      "epoch:  929  loss:  1.7392456531524658\n",
      "epoch:  930  loss:  1.739201307296753\n",
      "epoch:  931  loss:  1.7390600442886353\n",
      "epoch:  932  loss:  1.7388919591903687\n",
      "epoch:  933  loss:  1.7385830879211426\n",
      "epoch:  934  loss:  1.7381961345672607\n",
      "epoch:  935  loss:  1.7378157377243042\n",
      "epoch:  936  loss:  1.7373641729354858\n",
      "epoch:  937  loss:  1.737082600593567\n",
      "epoch:  938  loss:  1.7366896867752075\n",
      "epoch:  939  loss:  1.736436128616333\n",
      "epoch:  940  loss:  1.7361197471618652\n",
      "epoch:  941  loss:  1.735877513885498\n",
      "epoch:  942  loss:  1.7356077432632446\n",
      "epoch:  943  loss:  1.7353911399841309\n",
      "epoch:  944  loss:  1.735159993171692\n",
      "epoch:  945  loss:  1.7349755764007568\n",
      "epoch:  946  loss:  1.7348519563674927\n",
      "epoch:  947  loss:  1.7346858978271484\n",
      "epoch:  948  loss:  1.7346678972244263\n",
      "epoch:  949  loss:  1.7343305349349976\n",
      "epoch:  950  loss:  1.734175682067871\n",
      "epoch:  951  loss:  1.7336115837097168\n",
      "epoch:  952  loss:  1.7332074642181396\n",
      "epoch:  953  loss:  1.7327516078948975\n",
      "epoch:  954  loss:  1.7324005365371704\n",
      "epoch:  955  loss:  1.7321252822875977\n",
      "epoch:  956  loss:  1.7319034337997437\n",
      "epoch:  957  loss:  1.731722116470337\n",
      "epoch:  958  loss:  1.7315598726272583\n",
      "epoch:  959  loss:  1.7314952611923218\n",
      "epoch:  960  loss:  1.731286883354187\n",
      "epoch:  961  loss:  1.731286644935608\n",
      "epoch:  962  loss:  1.730872392654419\n",
      "epoch:  963  loss:  1.730668067932129\n",
      "epoch:  964  loss:  1.730202317237854\n",
      "epoch:  965  loss:  1.729891061782837\n",
      "epoch:  966  loss:  1.7296162843704224\n",
      "epoch:  967  loss:  1.7294046878814697\n",
      "epoch:  968  loss:  1.7293000221252441\n",
      "epoch:  969  loss:  1.729126214981079\n",
      "epoch:  970  loss:  1.7290656566619873\n",
      "epoch:  971  loss:  1.7287627458572388\n",
      "epoch:  972  loss:  1.7284821271896362\n",
      "epoch:  973  loss:  1.7279973030090332\n",
      "epoch:  974  loss:  1.7275530099868774\n",
      "epoch:  975  loss:  1.7271358966827393\n",
      "epoch:  976  loss:  1.7268136739730835\n",
      "epoch:  977  loss:  1.726578950881958\n",
      "epoch:  978  loss:  1.7264080047607422\n",
      "epoch:  979  loss:  1.7262920141220093\n",
      "epoch:  980  loss:  1.7261563539505005\n",
      "epoch:  981  loss:  1.7260640859603882\n",
      "epoch:  982  loss:  1.7258023023605347\n",
      "epoch:  983  loss:  1.7255743741989136\n",
      "epoch:  984  loss:  1.725165605545044\n",
      "epoch:  985  loss:  1.7248207330703735\n",
      "epoch:  986  loss:  1.7244466543197632\n",
      "epoch:  987  loss:  1.724181890487671\n",
      "epoch:  988  loss:  1.7239930629730225\n",
      "epoch:  989  loss:  1.7238513231277466\n",
      "epoch:  990  loss:  1.7239277362823486\n",
      "epoch:  991  loss:  1.7237321138381958\n",
      "epoch:  992  loss:  1.7239494323730469\n",
      "epoch:  993  loss:  1.7235037088394165\n",
      "epoch:  994  loss:  1.7234101295471191\n",
      "epoch:  995  loss:  1.7230111360549927\n",
      "epoch:  996  loss:  1.7227544784545898\n",
      "epoch:  997  loss:  1.7225066423416138\n",
      "epoch:  998  loss:  1.7221148014068604\n",
      "epoch:  999  loss:  1.7219020128250122\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    y_pred = model(x_train_smol)\n",
    "    loss = loss_fn(y_pred, y_train_smol)\n",
    "    print('epoch: ', epoch, ' loss: ', loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_smol, y_test_smol = get_data('./data/aclImdb/test/labeledBow.feat', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.278875\n"
     ]
    }
   ],
   "source": [
    "y_pred = model(x_test_smol)\n",
    "labels_pred = torch.argmax(y_pred, 1)\n",
    "correct = (labels_pred == y_test_smol).sum().item()\n",
    "print('Accuracy: ' + str(correct / len(y_test_smol)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
