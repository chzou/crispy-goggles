{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/aclImdb/imdb.vocab', 'r') as f:\n",
    "    vocab = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LEN = len(vocab)\n",
    "NUM_CLASSES = 10\n",
    "NUM_EXAMPLES = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(n):\n",
    "    arr = np.zeros(NUM_CLASSES)\n",
    "    arr[int(n)-1] = 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove2dict(src_filename):\n",
    "    data = {}\n",
    "    with open(src_filename) as f:\n",
    "        while True:\n",
    "            try:\n",
    "                line = next(f)\n",
    "                line = line.strip().split()\n",
    "                data[line[0]] = np.array(line[1: ], dtype=np.float)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "glove_dim = 300\n",
    "GLOVE = glove2dict('./data/glove.6B/glove.6B.{}d.txt'.format(glove_dim))\n",
    "\n",
    "def randvec(n=50, lower=-1.0, upper=1.0):\n",
    "    \"\"\"Returns a random vector of length `n`. `w` is ignored.\"\"\"\n",
    "    return np.array([random.uniform(lower, upper) for i in range(n)])\n",
    "\n",
    "def glove_vec(w):\n",
    "    \"\"\"Return `w`'s GloVe representation if available, else return \n",
    "    a random vector.\"\"\"\n",
    "    return GLOVE.get(w, randvec(glove_dim))\n",
    "\n",
    "def vec_average(u, v):\n",
    "    \"\"\"Averages np.array instances `u` and `v` into a new np.array\"\"\"\n",
    "    return np.add(u, v) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_to_vec(features):\n",
    "    all_word_vecs = []\n",
    "    for f in features:\n",
    "        i, c = f.split(':')    # index, count\n",
    "        w = vocab[int(i)]      # get vocab word by index\n",
    "        g = glove_vec(w)       # glove word embedding\n",
    "        all_word_vecs.append(g)\n",
    "    output = np.mean(all_word_vecs, axis=0)    \n",
    "#     arr = np.zeros(VOCAB_LEN)\n",
    "#     for f in features:\n",
    "#         i, c = f.split(':') # index, count\n",
    "#         arr[int(i)] = int(c)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_data(filename, num_examples):\n",
    "    with open(filename, 'r') as f:\n",
    "        imdb = f.readlines()\n",
    "    \n",
    "    x_train, y_train = [], []\n",
    "    label_count = defaultdict(int) # used to balance dataset\n",
    "    for line in imdb:\n",
    "        label, *features = line.split(' ')\n",
    "        if label_count[label] >= NUM_EXAMPLES / NUM_CLASSES:\n",
    "            continue\n",
    "        x_train.append(bow_to_vec(features))\n",
    "        y_train.append(int(label) - 1)\n",
    "        label_count[label] += 1\n",
    "    \n",
    "    x_train = torch.tensor(x_train, dtype=torch.float)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_smol, y_train_smol = get_data('./data/aclImdb/train/labeledBow.feat', NUM_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in, n_h, n_out = glove_dim, NUM_EXAMPLES, NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(n_in, n_h),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(n_h, n_out),\n",
    "                     nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  2.302448034286499\n",
      "epoch:  1  loss:  2.2842350006103516\n",
      "epoch:  2  loss:  2.2670977115631104\n",
      "epoch:  3  loss:  2.250572919845581\n",
      "epoch:  4  loss:  2.2347233295440674\n",
      "epoch:  5  loss:  2.2199411392211914\n",
      "epoch:  6  loss:  2.206761360168457\n",
      "epoch:  7  loss:  2.195650100708008\n",
      "epoch:  8  loss:  2.186852216720581\n",
      "epoch:  9  loss:  2.1802971363067627\n",
      "epoch:  10  loss:  2.175677537918091\n",
      "epoch:  11  loss:  2.172576665878296\n",
      "epoch:  12  loss:  2.1705641746520996\n",
      "epoch:  13  loss:  2.1692888736724854\n",
      "epoch:  14  loss:  2.1684906482696533\n",
      "epoch:  15  loss:  2.1679940223693848\n",
      "epoch:  16  loss:  2.1676816940307617\n",
      "epoch:  17  loss:  2.167489528656006\n",
      "epoch:  18  loss:  2.1673688888549805\n",
      "epoch:  19  loss:  2.167290687561035\n",
      "epoch:  20  loss:  2.1672394275665283\n",
      "epoch:  21  loss:  2.1672046184539795\n",
      "epoch:  22  loss:  2.167182683944702\n",
      "epoch:  23  loss:  2.1671621799468994\n",
      "epoch:  24  loss:  2.167142391204834\n",
      "epoch:  25  loss:  2.1671228408813477\n",
      "epoch:  26  loss:  2.167099714279175\n",
      "epoch:  27  loss:  2.167073965072632\n",
      "epoch:  28  loss:  2.1670432090759277\n",
      "epoch:  29  loss:  2.1670022010803223\n",
      "epoch:  30  loss:  2.1669466495513916\n",
      "epoch:  31  loss:  2.1668765544891357\n",
      "epoch:  32  loss:  2.166783332824707\n",
      "epoch:  33  loss:  2.1666555404663086\n",
      "epoch:  34  loss:  2.166470527648926\n",
      "epoch:  35  loss:  2.166207790374756\n",
      "epoch:  36  loss:  2.1658177375793457\n",
      "epoch:  37  loss:  2.165245771408081\n",
      "epoch:  38  loss:  2.1644628047943115\n",
      "epoch:  39  loss:  2.1635971069335938\n",
      "epoch:  40  loss:  2.163205862045288\n",
      "epoch:  41  loss:  2.1634469032287598\n",
      "epoch:  42  loss:  2.162757635116577\n",
      "epoch:  43  loss:  2.161508321762085\n",
      "epoch:  44  loss:  2.1605947017669678\n",
      "epoch:  45  loss:  2.159942388534546\n",
      "epoch:  46  loss:  2.1590445041656494\n",
      "epoch:  47  loss:  2.157546043395996\n",
      "epoch:  48  loss:  2.15535306930542\n",
      "epoch:  49  loss:  2.152981996536255\n",
      "epoch:  50  loss:  2.1519744396209717\n",
      "epoch:  51  loss:  2.151963233947754\n",
      "epoch:  52  loss:  2.1500742435455322\n",
      "epoch:  53  loss:  2.148022174835205\n",
      "epoch:  54  loss:  2.1468894481658936\n",
      "epoch:  55  loss:  2.1458375453948975\n",
      "epoch:  56  loss:  2.144359588623047\n",
      "epoch:  57  loss:  2.142655849456787\n",
      "epoch:  58  loss:  2.141099691390991\n",
      "epoch:  59  loss:  2.13978910446167\n",
      "epoch:  60  loss:  2.138322353363037\n",
      "epoch:  61  loss:  2.136475086212158\n",
      "epoch:  62  loss:  2.134704113006592\n",
      "epoch:  63  loss:  2.133460283279419\n",
      "epoch:  64  loss:  2.132209539413452\n",
      "epoch:  65  loss:  2.1303977966308594\n",
      "epoch:  66  loss:  2.1285793781280518\n",
      "epoch:  67  loss:  2.1271555423736572\n",
      "epoch:  68  loss:  2.1258208751678467\n",
      "epoch:  69  loss:  2.124246120452881\n",
      "epoch:  70  loss:  2.1226372718811035\n",
      "epoch:  71  loss:  2.1213178634643555\n",
      "epoch:  72  loss:  2.119969606399536\n",
      "epoch:  73  loss:  2.118380308151245\n",
      "epoch:  74  loss:  2.1169354915618896\n",
      "epoch:  75  loss:  2.1157050132751465\n",
      "epoch:  76  loss:  2.1143746376037598\n",
      "epoch:  77  loss:  2.112919807434082\n",
      "epoch:  78  loss:  2.1115775108337402\n",
      "epoch:  79  loss:  2.1103007793426514\n",
      "epoch:  80  loss:  2.108884334564209\n",
      "epoch:  81  loss:  2.1074304580688477\n",
      "epoch:  82  loss:  2.1060190200805664\n",
      "epoch:  83  loss:  2.1044511795043945\n",
      "epoch:  84  loss:  2.10261607170105\n",
      "epoch:  85  loss:  2.1005232334136963\n",
      "epoch:  86  loss:  2.0979878902435303\n",
      "epoch:  87  loss:  2.095028877258301\n",
      "epoch:  88  loss:  2.092531204223633\n",
      "epoch:  89  loss:  2.0911993980407715\n",
      "epoch:  90  loss:  2.0890958309173584\n",
      "epoch:  91  loss:  2.0865306854248047\n",
      "epoch:  92  loss:  2.085020065307617\n",
      "epoch:  93  loss:  2.083623170852661\n",
      "epoch:  94  loss:  2.081700086593628\n",
      "epoch:  95  loss:  2.0797934532165527\n",
      "epoch:  96  loss:  2.0781242847442627\n",
      "epoch:  97  loss:  2.0765397548675537\n",
      "epoch:  98  loss:  2.074927806854248\n",
      "epoch:  99  loss:  2.073221206665039\n",
      "epoch:  100  loss:  2.0714995861053467\n",
      "epoch:  101  loss:  2.069929599761963\n",
      "epoch:  102  loss:  2.0684523582458496\n",
      "epoch:  103  loss:  2.0669784545898438\n",
      "epoch:  104  loss:  2.0655629634857178\n",
      "epoch:  105  loss:  2.064164161682129\n",
      "epoch:  106  loss:  2.0627639293670654\n",
      "epoch:  107  loss:  2.061432361602783\n",
      "epoch:  108  loss:  2.0601696968078613\n",
      "epoch:  109  loss:  2.058946132659912\n",
      "epoch:  110  loss:  2.0577521324157715\n",
      "epoch:  111  loss:  2.056549310684204\n",
      "epoch:  112  loss:  2.0553696155548096\n",
      "epoch:  113  loss:  2.0542614459991455\n",
      "epoch:  114  loss:  2.0531888008117676\n",
      "epoch:  115  loss:  2.0521416664123535\n",
      "epoch:  116  loss:  2.051114082336426\n",
      "epoch:  117  loss:  2.050089120864868\n",
      "epoch:  118  loss:  2.049086570739746\n",
      "epoch:  119  loss:  2.0481107234954834\n",
      "epoch:  120  loss:  2.0471627712249756\n",
      "epoch:  121  loss:  2.046247720718384\n",
      "epoch:  122  loss:  2.0453407764434814\n",
      "epoch:  123  loss:  2.0444345474243164\n",
      "epoch:  124  loss:  2.043531894683838\n",
      "epoch:  125  loss:  2.0426371097564697\n",
      "epoch:  126  loss:  2.0417675971984863\n",
      "epoch:  127  loss:  2.040903091430664\n",
      "epoch:  128  loss:  2.040032148361206\n",
      "epoch:  129  loss:  2.0391342639923096\n",
      "epoch:  130  loss:  2.038191318511963\n",
      "epoch:  131  loss:  2.0371735095977783\n",
      "epoch:  132  loss:  2.036004066467285\n",
      "epoch:  133  loss:  2.0345211029052734\n",
      "epoch:  134  loss:  2.0323808193206787\n",
      "epoch:  135  loss:  2.0291402339935303\n",
      "epoch:  136  loss:  2.025289535522461\n",
      "epoch:  137  loss:  2.0241291522979736\n",
      "epoch:  138  loss:  2.024481773376465\n",
      "epoch:  139  loss:  2.0226781368255615\n",
      "epoch:  140  loss:  2.019458293914795\n",
      "epoch:  141  loss:  2.0172693729400635\n",
      "epoch:  142  loss:  2.0162785053253174\n",
      "epoch:  143  loss:  2.0153167247772217\n",
      "epoch:  144  loss:  2.0140745639801025\n",
      "epoch:  145  loss:  2.012303590774536\n",
      "epoch:  146  loss:  2.0105504989624023\n",
      "epoch:  147  loss:  2.009141206741333\n",
      "epoch:  148  loss:  2.0081593990325928\n",
      "epoch:  149  loss:  2.0073537826538086\n",
      "epoch:  150  loss:  2.0062496662139893\n",
      "epoch:  151  loss:  2.005040168762207\n",
      "epoch:  152  loss:  2.003838062286377\n",
      "epoch:  153  loss:  2.0028672218322754\n",
      "epoch:  154  loss:  2.001952648162842\n",
      "epoch:  155  loss:  2.000997543334961\n",
      "epoch:  156  loss:  1.9999698400497437\n",
      "epoch:  157  loss:  1.9988563060760498\n",
      "epoch:  158  loss:  1.9975491762161255\n",
      "epoch:  159  loss:  1.995689868927002\n",
      "epoch:  160  loss:  1.9928467273712158\n",
      "epoch:  161  loss:  1.9884802103042603\n",
      "epoch:  162  loss:  1.9837945699691772\n",
      "epoch:  163  loss:  1.9830260276794434\n",
      "epoch:  164  loss:  1.9837037324905396\n",
      "epoch:  165  loss:  1.9817924499511719\n",
      "epoch:  166  loss:  1.97821843624115\n",
      "epoch:  167  loss:  1.9754786491394043\n",
      "epoch:  168  loss:  1.9742512702941895\n",
      "epoch:  169  loss:  1.9734857082366943\n",
      "epoch:  170  loss:  1.9722590446472168\n",
      "epoch:  171  loss:  1.9705402851104736\n",
      "epoch:  172  loss:  1.9686704874038696\n",
      "epoch:  173  loss:  1.9669723510742188\n",
      "epoch:  174  loss:  1.965630292892456\n",
      "epoch:  175  loss:  1.9645514488220215\n",
      "epoch:  176  loss:  1.9635765552520752\n",
      "epoch:  177  loss:  1.9625247716903687\n",
      "epoch:  178  loss:  1.9613010883331299\n",
      "epoch:  179  loss:  1.9599496126174927\n",
      "epoch:  180  loss:  1.9586366415023804\n",
      "epoch:  181  loss:  1.9574809074401855\n",
      "epoch:  182  loss:  1.9564082622528076\n",
      "epoch:  183  loss:  1.955114722251892\n",
      "epoch:  184  loss:  1.95320725440979\n",
      "epoch:  185  loss:  1.9503365755081177\n",
      "epoch:  186  loss:  1.9464998245239258\n",
      "epoch:  187  loss:  1.9433331489562988\n",
      "epoch:  188  loss:  1.9427746534347534\n",
      "epoch:  189  loss:  1.9422305822372437\n",
      "epoch:  190  loss:  1.9399614334106445\n",
      "epoch:  191  loss:  1.9369256496429443\n",
      "epoch:  192  loss:  1.9344854354858398\n",
      "epoch:  193  loss:  1.9329463243484497\n",
      "epoch:  194  loss:  1.931713581085205\n",
      "epoch:  195  loss:  1.930411696434021\n",
      "epoch:  196  loss:  1.9288849830627441\n",
      "epoch:  197  loss:  1.927115797996521\n",
      "epoch:  198  loss:  1.9253151416778564\n",
      "epoch:  199  loss:  1.923798680305481\n",
      "epoch:  200  loss:  1.9226799011230469\n",
      "epoch:  201  loss:  1.9217051267623901\n",
      "epoch:  202  loss:  1.9206122159957886\n",
      "epoch:  203  loss:  1.9193153381347656\n",
      "epoch:  204  loss:  1.9179719686508179\n",
      "epoch:  205  loss:  1.9167556762695312\n",
      "epoch:  206  loss:  1.9156948328018188\n",
      "epoch:  207  loss:  1.914689302444458\n",
      "epoch:  208  loss:  1.9136773347854614\n",
      "epoch:  209  loss:  1.9126766920089722\n",
      "epoch:  210  loss:  1.9116733074188232\n",
      "epoch:  211  loss:  1.9106149673461914\n",
      "epoch:  212  loss:  1.909541368484497\n",
      "epoch:  213  loss:  1.9085263013839722\n",
      "epoch:  214  loss:  1.907596230506897\n",
      "epoch:  215  loss:  1.9066842794418335\n",
      "epoch:  216  loss:  1.9057339429855347\n",
      "epoch:  217  loss:  1.9047653675079346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  218  loss:  1.9038013219833374\n",
      "epoch:  219  loss:  1.9028489589691162\n",
      "epoch:  220  loss:  1.9019027948379517\n",
      "epoch:  221  loss:  1.9009655714035034\n",
      "epoch:  222  loss:  1.9000545740127563\n",
      "epoch:  223  loss:  1.899138331413269\n",
      "epoch:  224  loss:  1.898209810256958\n",
      "epoch:  225  loss:  1.8972816467285156\n",
      "epoch:  226  loss:  1.896366000175476\n",
      "epoch:  227  loss:  1.8954493999481201\n",
      "epoch:  228  loss:  1.8945398330688477\n",
      "epoch:  229  loss:  1.89363694190979\n",
      "epoch:  230  loss:  1.8927361965179443\n",
      "epoch:  231  loss:  1.8918300867080688\n",
      "epoch:  232  loss:  1.8909207582473755\n",
      "epoch:  233  loss:  1.8900200128555298\n",
      "epoch:  234  loss:  1.8891302347183228\n",
      "epoch:  235  loss:  1.8882352113723755\n",
      "epoch:  236  loss:  1.8873395919799805\n",
      "epoch:  237  loss:  1.8864452838897705\n",
      "epoch:  238  loss:  1.8855538368225098\n",
      "epoch:  239  loss:  1.8846606016159058\n",
      "epoch:  240  loss:  1.8837765455245972\n",
      "epoch:  241  loss:  1.8828963041305542\n",
      "epoch:  242  loss:  1.8820136785507202\n",
      "epoch:  243  loss:  1.881127953529358\n",
      "epoch:  244  loss:  1.8802495002746582\n",
      "epoch:  245  loss:  1.879374384880066\n",
      "epoch:  246  loss:  1.8784987926483154\n",
      "epoch:  247  loss:  1.8776299953460693\n",
      "epoch:  248  loss:  1.8767614364624023\n",
      "epoch:  249  loss:  1.875892996788025\n",
      "epoch:  250  loss:  1.8750290870666504\n",
      "epoch:  251  loss:  1.874168038368225\n",
      "epoch:  252  loss:  1.8733118772506714\n",
      "epoch:  253  loss:  1.8724552392959595\n",
      "epoch:  254  loss:  1.8716020584106445\n",
      "epoch:  255  loss:  1.870752215385437\n",
      "epoch:  256  loss:  1.8699069023132324\n",
      "epoch:  257  loss:  1.8690636157989502\n",
      "epoch:  258  loss:  1.8682249784469604\n",
      "epoch:  259  loss:  1.8673853874206543\n",
      "epoch:  260  loss:  1.8665469884872437\n",
      "epoch:  261  loss:  1.8657156229019165\n",
      "epoch:  262  loss:  1.8648837804794312\n",
      "epoch:  263  loss:  1.864057183265686\n",
      "epoch:  264  loss:  1.8632274866104126\n",
      "epoch:  265  loss:  1.8624058961868286\n",
      "epoch:  266  loss:  1.8615819215774536\n",
      "epoch:  267  loss:  1.8607620000839233\n",
      "epoch:  268  loss:  1.8599449396133423\n",
      "epoch:  269  loss:  1.8591303825378418\n",
      "epoch:  270  loss:  1.8583182096481323\n",
      "epoch:  271  loss:  1.8575135469436646\n",
      "epoch:  272  loss:  1.8567209243774414\n",
      "epoch:  273  loss:  1.8559476137161255\n",
      "epoch:  274  loss:  1.855229377746582\n",
      "epoch:  275  loss:  1.854602336883545\n",
      "epoch:  276  loss:  1.8540277481079102\n",
      "epoch:  277  loss:  1.8533111810684204\n",
      "epoch:  278  loss:  1.8522170782089233\n",
      "epoch:  279  loss:  1.8512035608291626\n",
      "epoch:  280  loss:  1.8505973815917969\n",
      "epoch:  281  loss:  1.8500291109085083\n",
      "epoch:  282  loss:  1.8491427898406982\n",
      "epoch:  283  loss:  1.8482022285461426\n",
      "epoch:  284  loss:  1.8475768566131592\n",
      "epoch:  285  loss:  1.8469613790512085\n",
      "epoch:  286  loss:  1.8460818529129028\n",
      "epoch:  287  loss:  1.8452579975128174\n",
      "epoch:  288  loss:  1.844642996788025\n",
      "epoch:  289  loss:  1.8439457416534424\n",
      "epoch:  290  loss:  1.8431059122085571\n",
      "epoch:  291  loss:  1.8423690795898438\n",
      "epoch:  292  loss:  1.8417363166809082\n",
      "epoch:  293  loss:  1.840999960899353\n",
      "epoch:  294  loss:  1.8402100801467896\n",
      "epoch:  295  loss:  1.839513897895813\n",
      "epoch:  296  loss:  1.8388562202453613\n",
      "epoch:  297  loss:  1.8381119966506958\n",
      "epoch:  298  loss:  1.8373645544052124\n",
      "epoch:  299  loss:  1.8366793394088745\n",
      "epoch:  300  loss:  1.8360034227371216\n",
      "epoch:  301  loss:  1.8352768421173096\n",
      "epoch:  302  loss:  1.8345471620559692\n",
      "epoch:  303  loss:  1.833864450454712\n",
      "epoch:  304  loss:  1.8331893682479858\n",
      "epoch:  305  loss:  1.83247709274292\n",
      "epoch:  306  loss:  1.8317620754241943\n",
      "epoch:  307  loss:  1.8310797214508057\n",
      "epoch:  308  loss:  1.830405831336975\n",
      "epoch:  309  loss:  1.8297184705734253\n",
      "epoch:  310  loss:  1.829020380973816\n",
      "epoch:  311  loss:  1.8283324241638184\n",
      "epoch:  312  loss:  1.8276638984680176\n",
      "epoch:  313  loss:  1.8269929885864258\n",
      "epoch:  314  loss:  1.8263112306594849\n",
      "epoch:  315  loss:  1.8256278038024902\n",
      "epoch:  316  loss:  1.8249526023864746\n",
      "epoch:  317  loss:  1.8242892026901245\n",
      "epoch:  318  loss:  1.8236240148544312\n",
      "epoch:  319  loss:  1.8229609727859497\n",
      "epoch:  320  loss:  1.8222886323928833\n",
      "epoch:  321  loss:  1.8216246366500854\n",
      "epoch:  322  loss:  1.8209649324417114\n",
      "epoch:  323  loss:  1.820307970046997\n",
      "epoch:  324  loss:  1.819654941558838\n",
      "epoch:  325  loss:  1.8190017938613892\n",
      "epoch:  326  loss:  1.8183495998382568\n",
      "epoch:  327  loss:  1.8176950216293335\n",
      "epoch:  328  loss:  1.8170411586761475\n",
      "epoch:  329  loss:  1.8163933753967285\n",
      "epoch:  330  loss:  1.8157455921173096\n",
      "epoch:  331  loss:  1.815100908279419\n",
      "epoch:  332  loss:  1.8144621849060059\n",
      "epoch:  333  loss:  1.813820242881775\n",
      "epoch:  334  loss:  1.8131849765777588\n",
      "epoch:  335  loss:  1.8125492334365845\n",
      "epoch:  336  loss:  1.8119151592254639\n",
      "epoch:  337  loss:  1.8112845420837402\n",
      "epoch:  338  loss:  1.8106553554534912\n",
      "epoch:  339  loss:  1.8100297451019287\n",
      "epoch:  340  loss:  1.8094069957733154\n",
      "epoch:  341  loss:  1.808787226676941\n",
      "epoch:  342  loss:  1.8081775903701782\n",
      "epoch:  343  loss:  1.807570457458496\n",
      "epoch:  344  loss:  1.8069638013839722\n",
      "epoch:  345  loss:  1.806363821029663\n",
      "epoch:  346  loss:  1.805749535560608\n",
      "epoch:  347  loss:  1.8051282167434692\n",
      "epoch:  348  loss:  1.8044898509979248\n",
      "epoch:  349  loss:  1.80385160446167\n",
      "epoch:  350  loss:  1.8032056093215942\n",
      "epoch:  351  loss:  1.8025703430175781\n",
      "epoch:  352  loss:  1.8019471168518066\n",
      "epoch:  353  loss:  1.8013370037078857\n",
      "epoch:  354  loss:  1.8007333278656006\n",
      "epoch:  355  loss:  1.800134539604187\n",
      "epoch:  356  loss:  1.7995470762252808\n",
      "epoch:  357  loss:  1.7989683151245117\n",
      "epoch:  358  loss:  1.7984040975570679\n",
      "epoch:  359  loss:  1.797857403755188\n",
      "epoch:  360  loss:  1.7973212003707886\n",
      "epoch:  361  loss:  1.796782374382019\n",
      "epoch:  362  loss:  1.7962228059768677\n",
      "epoch:  363  loss:  1.7955976724624634\n",
      "epoch:  364  loss:  1.7949339151382446\n",
      "epoch:  365  loss:  1.7942639589309692\n",
      "epoch:  366  loss:  1.7936484813690186\n",
      "epoch:  367  loss:  1.7930947542190552\n",
      "epoch:  368  loss:  1.7925816774368286\n",
      "epoch:  369  loss:  1.792076587677002\n",
      "epoch:  370  loss:  1.7915436029434204\n",
      "epoch:  371  loss:  1.790967583656311\n",
      "epoch:  372  loss:  1.790350079536438\n",
      "epoch:  373  loss:  1.7897353172302246\n",
      "epoch:  374  loss:  1.7891560792922974\n",
      "epoch:  375  loss:  1.7886152267456055\n",
      "epoch:  376  loss:  1.7881008386611938\n",
      "epoch:  377  loss:  1.7875896692276\n",
      "epoch:  378  loss:  1.7870683670043945\n",
      "epoch:  379  loss:  1.786514163017273\n",
      "epoch:  380  loss:  1.785942554473877\n",
      "epoch:  381  loss:  1.7853676080703735\n",
      "epoch:  382  loss:  1.7848080396652222\n",
      "epoch:  383  loss:  1.784264087677002\n",
      "epoch:  384  loss:  1.7837400436401367\n",
      "epoch:  385  loss:  1.7832261323928833\n",
      "epoch:  386  loss:  1.782715916633606\n",
      "epoch:  387  loss:  1.782199501991272\n",
      "epoch:  388  loss:  1.781678318977356\n",
      "epoch:  389  loss:  1.781152606010437\n",
      "epoch:  390  loss:  1.7806135416030884\n",
      "epoch:  391  loss:  1.780073642730713\n",
      "epoch:  392  loss:  1.7795361280441284\n",
      "epoch:  393  loss:  1.7789992094039917\n",
      "epoch:  394  loss:  1.7784721851348877\n",
      "epoch:  395  loss:  1.7779446840286255\n",
      "epoch:  396  loss:  1.7774254083633423\n",
      "epoch:  397  loss:  1.7769078016281128\n",
      "epoch:  398  loss:  1.776395320892334\n",
      "epoch:  399  loss:  1.7758833169937134\n",
      "epoch:  400  loss:  1.7753793001174927\n",
      "epoch:  401  loss:  1.7748793363571167\n",
      "epoch:  402  loss:  1.7743927240371704\n",
      "epoch:  403  loss:  1.7739158868789673\n",
      "epoch:  404  loss:  1.7734637260437012\n",
      "epoch:  405  loss:  1.773026943206787\n",
      "epoch:  406  loss:  1.772610068321228\n",
      "epoch:  407  loss:  1.7721610069274902\n",
      "epoch:  408  loss:  1.771663784980774\n",
      "epoch:  409  loss:  1.771061658859253\n",
      "epoch:  410  loss:  1.7704356908798218\n",
      "epoch:  411  loss:  1.769852876663208\n",
      "epoch:  412  loss:  1.7693613767623901\n",
      "epoch:  413  loss:  1.768941044807434\n",
      "epoch:  414  loss:  1.7685365676879883\n",
      "epoch:  415  loss:  1.7680940628051758\n",
      "epoch:  416  loss:  1.7675775289535522\n",
      "epoch:  417  loss:  1.767020583152771\n",
      "epoch:  418  loss:  1.7664722204208374\n",
      "epoch:  419  loss:  1.765982985496521\n",
      "epoch:  420  loss:  1.7655401229858398\n",
      "epoch:  421  loss:  1.7651172876358032\n",
      "epoch:  422  loss:  1.764674186706543\n",
      "epoch:  423  loss:  1.7641884088516235\n",
      "epoch:  424  loss:  1.7636812925338745\n",
      "epoch:  425  loss:  1.7631721496582031\n",
      "epoch:  426  loss:  1.7626878023147583\n",
      "epoch:  427  loss:  1.7622298002243042\n",
      "epoch:  428  loss:  1.7617930173873901\n",
      "epoch:  429  loss:  1.7613532543182373\n",
      "epoch:  430  loss:  1.7609080076217651\n",
      "epoch:  431  loss:  1.7604411840438843\n",
      "epoch:  432  loss:  1.759959101676941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  433  loss:  1.7594785690307617\n",
      "epoch:  434  loss:  1.7590011358261108\n",
      "epoch:  435  loss:  1.7585376501083374\n",
      "epoch:  436  loss:  1.7580877542495728\n",
      "epoch:  437  loss:  1.7576411962509155\n",
      "epoch:  438  loss:  1.7572035789489746\n",
      "epoch:  439  loss:  1.7567652463912964\n",
      "epoch:  440  loss:  1.756325125694275\n",
      "epoch:  441  loss:  1.7558822631835938\n",
      "epoch:  442  loss:  1.755433201789856\n",
      "epoch:  443  loss:  1.7549787759780884\n",
      "epoch:  444  loss:  1.7545243501663208\n",
      "epoch:  445  loss:  1.7540676593780518\n",
      "epoch:  446  loss:  1.7536147832870483\n",
      "epoch:  447  loss:  1.7531635761260986\n",
      "epoch:  448  loss:  1.7527189254760742\n",
      "epoch:  449  loss:  1.7522735595703125\n",
      "epoch:  450  loss:  1.7518330812454224\n",
      "epoch:  451  loss:  1.7513967752456665\n",
      "epoch:  452  loss:  1.75095534324646\n",
      "epoch:  453  loss:  1.7505203485488892\n",
      "epoch:  454  loss:  1.750085473060608\n",
      "epoch:  455  loss:  1.7496495246887207\n",
      "epoch:  456  loss:  1.7492165565490723\n",
      "epoch:  457  loss:  1.7487902641296387\n",
      "epoch:  458  loss:  1.7483642101287842\n",
      "epoch:  459  loss:  1.747949242591858\n",
      "epoch:  460  loss:  1.7475428581237793\n",
      "epoch:  461  loss:  1.747154712677002\n",
      "epoch:  462  loss:  1.746799111366272\n",
      "epoch:  463  loss:  1.7464669942855835\n",
      "epoch:  464  loss:  1.746157169342041\n",
      "epoch:  465  loss:  1.745784044265747\n",
      "epoch:  466  loss:  1.7453335523605347\n",
      "epoch:  467  loss:  1.7447600364685059\n",
      "epoch:  468  loss:  1.7441929578781128\n",
      "epoch:  469  loss:  1.7437182664871216\n",
      "epoch:  470  loss:  1.7433559894561768\n",
      "epoch:  471  loss:  1.7430510520935059\n",
      "epoch:  472  loss:  1.7427151203155518\n",
      "epoch:  473  loss:  1.7423033714294434\n",
      "epoch:  474  loss:  1.7417982816696167\n",
      "epoch:  475  loss:  1.7412958145141602\n",
      "epoch:  476  loss:  1.74086332321167\n",
      "epoch:  477  loss:  1.740501046180725\n",
      "epoch:  478  loss:  1.7401628494262695\n",
      "epoch:  479  loss:  1.7397902011871338\n",
      "epoch:  480  loss:  1.739365577697754\n",
      "epoch:  481  loss:  1.7389073371887207\n",
      "epoch:  482  loss:  1.7384648323059082\n",
      "epoch:  483  loss:  1.7380661964416504\n",
      "epoch:  484  loss:  1.7377046346664429\n",
      "epoch:  485  loss:  1.7373437881469727\n",
      "epoch:  486  loss:  1.7369589805603027\n",
      "epoch:  487  loss:  1.7365459203720093\n",
      "epoch:  488  loss:  1.7361152172088623\n",
      "epoch:  489  loss:  1.7356986999511719\n",
      "epoch:  490  loss:  1.7353041172027588\n",
      "epoch:  491  loss:  1.734929084777832\n",
      "epoch:  492  loss:  1.734559178352356\n",
      "epoch:  493  loss:  1.7341879606246948\n",
      "epoch:  494  loss:  1.7338017225265503\n",
      "epoch:  495  loss:  1.7334002256393433\n",
      "epoch:  496  loss:  1.732994556427002\n",
      "epoch:  497  loss:  1.7325940132141113\n",
      "epoch:  498  loss:  1.732201099395752\n",
      "epoch:  499  loss:  1.7318185567855835\n",
      "epoch:  500  loss:  1.7314460277557373\n",
      "epoch:  501  loss:  1.7310736179351807\n",
      "epoch:  502  loss:  1.7307029962539673\n",
      "epoch:  503  loss:  1.730329990386963\n",
      "epoch:  504  loss:  1.7299532890319824\n",
      "epoch:  505  loss:  1.7295674085617065\n",
      "epoch:  506  loss:  1.7291808128356934\n",
      "epoch:  507  loss:  1.7287931442260742\n",
      "epoch:  508  loss:  1.7284088134765625\n",
      "epoch:  509  loss:  1.7280234098434448\n",
      "epoch:  510  loss:  1.7276400327682495\n",
      "epoch:  511  loss:  1.7272632122039795\n",
      "epoch:  512  loss:  1.7268836498260498\n",
      "epoch:  513  loss:  1.7265068292617798\n",
      "epoch:  514  loss:  1.7261319160461426\n",
      "epoch:  515  loss:  1.7257541418075562\n",
      "epoch:  516  loss:  1.7253750562667847\n",
      "epoch:  517  loss:  1.725000023841858\n",
      "epoch:  518  loss:  1.7246222496032715\n",
      "epoch:  519  loss:  1.7242485284805298\n",
      "epoch:  520  loss:  1.7238709926605225\n",
      "epoch:  521  loss:  1.7234972715377808\n",
      "epoch:  522  loss:  1.7231309413909912\n",
      "epoch:  523  loss:  1.7227745056152344\n",
      "epoch:  524  loss:  1.722428798675537\n",
      "epoch:  525  loss:  1.7221035957336426\n",
      "epoch:  526  loss:  1.7217962741851807\n",
      "epoch:  527  loss:  1.7214860916137695\n",
      "epoch:  528  loss:  1.7211718559265137\n",
      "epoch:  529  loss:  1.720780372619629\n",
      "epoch:  530  loss:  1.720333218574524\n",
      "epoch:  531  loss:  1.719852089881897\n",
      "epoch:  532  loss:  1.719401240348816\n",
      "epoch:  533  loss:  1.7190165519714355\n",
      "epoch:  534  loss:  1.7186977863311768\n",
      "epoch:  535  loss:  1.718397855758667\n",
      "epoch:  536  loss:  1.7180770635604858\n",
      "epoch:  537  loss:  1.7177194356918335\n",
      "epoch:  538  loss:  1.7173062562942505\n",
      "epoch:  539  loss:  1.7168738842010498\n",
      "epoch:  540  loss:  1.7164615392684937\n",
      "epoch:  541  loss:  1.716094970703125\n",
      "epoch:  542  loss:  1.7157667875289917\n",
      "epoch:  543  loss:  1.715447187423706\n",
      "epoch:  544  loss:  1.7151166200637817\n",
      "epoch:  545  loss:  1.7147542238235474\n",
      "epoch:  546  loss:  1.7143659591674805\n",
      "epoch:  547  loss:  1.7139770984649658\n",
      "epoch:  548  loss:  1.713602900505066\n",
      "epoch:  549  loss:  1.7132552862167358\n",
      "epoch:  550  loss:  1.7129242420196533\n",
      "epoch:  551  loss:  1.7125983238220215\n",
      "epoch:  552  loss:  1.7122642993927002\n",
      "epoch:  553  loss:  1.711917519569397\n",
      "epoch:  554  loss:  1.7115504741668701\n",
      "epoch:  555  loss:  1.7111858129501343\n",
      "epoch:  556  loss:  1.7108261585235596\n",
      "epoch:  557  loss:  1.7104814052581787\n",
      "epoch:  558  loss:  1.7101495265960693\n",
      "epoch:  559  loss:  1.7098182439804077\n",
      "epoch:  560  loss:  1.7094929218292236\n",
      "epoch:  561  loss:  1.709157109260559\n",
      "epoch:  562  loss:  1.7088181972503662\n",
      "epoch:  563  loss:  1.7084765434265137\n",
      "epoch:  564  loss:  1.7081338167190552\n",
      "epoch:  565  loss:  1.7077927589416504\n",
      "epoch:  566  loss:  1.7074532508850098\n",
      "epoch:  567  loss:  1.7071223258972168\n",
      "epoch:  568  loss:  1.7067965269088745\n",
      "epoch:  569  loss:  1.7064727544784546\n",
      "epoch:  570  loss:  1.7061514854431152\n",
      "epoch:  571  loss:  1.7058312892913818\n",
      "epoch:  572  loss:  1.7055131196975708\n",
      "epoch:  573  loss:  1.705194115638733\n",
      "epoch:  574  loss:  1.7048667669296265\n",
      "epoch:  575  loss:  1.7045466899871826\n",
      "epoch:  576  loss:  1.7042129039764404\n",
      "epoch:  577  loss:  1.7038888931274414\n",
      "epoch:  578  loss:  1.7035720348358154\n",
      "epoch:  579  loss:  1.7032641172409058\n",
      "epoch:  580  loss:  1.7029814720153809\n",
      "epoch:  581  loss:  1.7027182579040527\n",
      "epoch:  582  loss:  1.7024873495101929\n",
      "epoch:  583  loss:  1.702255129814148\n",
      "epoch:  584  loss:  1.7019965648651123\n",
      "epoch:  585  loss:  1.7016607522964478\n",
      "epoch:  586  loss:  1.7012661695480347\n",
      "epoch:  587  loss:  1.7008625268936157\n",
      "epoch:  588  loss:  1.7005009651184082\n",
      "epoch:  589  loss:  1.7001869678497314\n",
      "epoch:  590  loss:  1.699904441833496\n",
      "epoch:  591  loss:  1.699642539024353\n",
      "epoch:  592  loss:  1.6993845701217651\n",
      "epoch:  593  loss:  1.6990962028503418\n",
      "epoch:  594  loss:  1.6987680196762085\n",
      "epoch:  595  loss:  1.6984062194824219\n",
      "epoch:  596  loss:  1.698062777519226\n",
      "epoch:  597  loss:  1.6977614164352417\n",
      "epoch:  598  loss:  1.697501540184021\n",
      "epoch:  599  loss:  1.6972498893737793\n",
      "epoch:  600  loss:  1.6969698667526245\n",
      "epoch:  601  loss:  1.6966661214828491\n",
      "epoch:  602  loss:  1.6963409185409546\n",
      "epoch:  603  loss:  1.6960246562957764\n",
      "epoch:  604  loss:  1.695737361907959\n",
      "epoch:  605  loss:  1.695462942123413\n",
      "epoch:  606  loss:  1.6951916217803955\n",
      "epoch:  607  loss:  1.6949046850204468\n",
      "epoch:  608  loss:  1.694615364074707\n",
      "epoch:  609  loss:  1.6943304538726807\n",
      "epoch:  610  loss:  1.6940521001815796\n",
      "epoch:  611  loss:  1.6937744617462158\n",
      "epoch:  612  loss:  1.693483829498291\n",
      "epoch:  613  loss:  1.6931971311569214\n",
      "epoch:  614  loss:  1.6929129362106323\n",
      "epoch:  615  loss:  1.6926301717758179\n",
      "epoch:  616  loss:  1.6923625469207764\n",
      "epoch:  617  loss:  1.6920983791351318\n",
      "epoch:  618  loss:  1.6918377876281738\n",
      "epoch:  619  loss:  1.6915744543075562\n",
      "epoch:  620  loss:  1.6913048028945923\n",
      "epoch:  621  loss:  1.6910327672958374\n",
      "epoch:  622  loss:  1.690757155418396\n",
      "epoch:  623  loss:  1.6904802322387695\n",
      "epoch:  624  loss:  1.6902084350585938\n",
      "epoch:  625  loss:  1.6899334192276\n",
      "epoch:  626  loss:  1.6896647214889526\n",
      "epoch:  627  loss:  1.6893947124481201\n",
      "epoch:  628  loss:  1.6891276836395264\n",
      "epoch:  629  loss:  1.6888635158538818\n",
      "epoch:  630  loss:  1.6885972023010254\n",
      "epoch:  631  loss:  1.6883333921432495\n",
      "epoch:  632  loss:  1.688072919845581\n",
      "epoch:  633  loss:  1.687807321548462\n",
      "epoch:  634  loss:  1.6875483989715576\n",
      "epoch:  635  loss:  1.6872880458831787\n",
      "epoch:  636  loss:  1.6870285272598267\n",
      "epoch:  637  loss:  1.6867730617523193\n",
      "epoch:  638  loss:  1.6865260601043701\n",
      "epoch:  639  loss:  1.6862826347351074\n",
      "epoch:  640  loss:  1.686047077178955\n",
      "epoch:  641  loss:  1.6858173608779907\n",
      "epoch:  642  loss:  1.685584306716919\n",
      "epoch:  643  loss:  1.6853339672088623\n",
      "epoch:  644  loss:  1.6850628852844238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  645  loss:  1.6847797632217407\n",
      "epoch:  646  loss:  1.6844990253448486\n",
      "epoch:  647  loss:  1.684220790863037\n",
      "epoch:  648  loss:  1.6839568614959717\n",
      "epoch:  649  loss:  1.6837029457092285\n",
      "epoch:  650  loss:  1.6834557056427002\n",
      "epoch:  651  loss:  1.6832166910171509\n",
      "epoch:  652  loss:  1.682981252670288\n",
      "epoch:  653  loss:  1.6827497482299805\n",
      "epoch:  654  loss:  1.6825120449066162\n",
      "epoch:  655  loss:  1.6822677850723267\n",
      "epoch:  656  loss:  1.6820124387741089\n",
      "epoch:  657  loss:  1.6817502975463867\n",
      "epoch:  658  loss:  1.681488037109375\n",
      "epoch:  659  loss:  1.6812282800674438\n",
      "epoch:  660  loss:  1.680972695350647\n",
      "epoch:  661  loss:  1.68072509765625\n",
      "epoch:  662  loss:  1.680483102798462\n",
      "epoch:  663  loss:  1.680240511894226\n",
      "epoch:  664  loss:  1.6800003051757812\n",
      "epoch:  665  loss:  1.6797668933868408\n",
      "epoch:  666  loss:  1.6795331239700317\n",
      "epoch:  667  loss:  1.6793023347854614\n",
      "epoch:  668  loss:  1.679073691368103\n",
      "epoch:  669  loss:  1.6788409948349\n",
      "epoch:  670  loss:  1.6786036491394043\n",
      "epoch:  671  loss:  1.6783665418624878\n",
      "epoch:  672  loss:  1.6781244277954102\n",
      "epoch:  673  loss:  1.6778874397277832\n",
      "epoch:  674  loss:  1.6776524782180786\n",
      "epoch:  675  loss:  1.677426815032959\n",
      "epoch:  676  loss:  1.6771979331970215\n",
      "epoch:  677  loss:  1.676965594291687\n",
      "epoch:  678  loss:  1.6767354011535645\n",
      "epoch:  679  loss:  1.6764960289001465\n",
      "epoch:  680  loss:  1.6762570142745972\n",
      "epoch:  681  loss:  1.6760097742080688\n",
      "epoch:  682  loss:  1.6757681369781494\n",
      "epoch:  683  loss:  1.6755322217941284\n",
      "epoch:  684  loss:  1.675298810005188\n",
      "epoch:  685  loss:  1.67507004737854\n",
      "epoch:  686  loss:  1.6748461723327637\n",
      "epoch:  687  loss:  1.6746184825897217\n",
      "epoch:  688  loss:  1.6743905544281006\n",
      "epoch:  689  loss:  1.6741620302200317\n",
      "epoch:  690  loss:  1.6739314794540405\n",
      "epoch:  691  loss:  1.6737045049667358\n",
      "epoch:  692  loss:  1.673478603363037\n",
      "epoch:  693  loss:  1.6732593774795532\n",
      "epoch:  694  loss:  1.6730388402938843\n",
      "epoch:  695  loss:  1.672829031944275\n",
      "epoch:  696  loss:  1.6726211309432983\n",
      "epoch:  697  loss:  1.6724188327789307\n",
      "epoch:  698  loss:  1.6722204685211182\n",
      "epoch:  699  loss:  1.6720203161239624\n",
      "epoch:  700  loss:  1.671821117401123\n",
      "epoch:  701  loss:  1.671613335609436\n",
      "epoch:  702  loss:  1.6713957786560059\n",
      "epoch:  703  loss:  1.6711666584014893\n",
      "epoch:  704  loss:  1.6709296703338623\n",
      "epoch:  705  loss:  1.6706854104995728\n",
      "epoch:  706  loss:  1.6704515218734741\n",
      "epoch:  707  loss:  1.670230507850647\n",
      "epoch:  708  loss:  1.6700206995010376\n",
      "epoch:  709  loss:  1.6698181629180908\n",
      "epoch:  710  loss:  1.6696261167526245\n",
      "epoch:  711  loss:  1.6694400310516357\n",
      "epoch:  712  loss:  1.6692473888397217\n",
      "epoch:  713  loss:  1.6690537929534912\n",
      "epoch:  714  loss:  1.6688443422317505\n",
      "epoch:  715  loss:  1.6686335802078247\n",
      "epoch:  716  loss:  1.6684150695800781\n",
      "epoch:  717  loss:  1.6681965589523315\n",
      "epoch:  718  loss:  1.667985200881958\n",
      "epoch:  719  loss:  1.6677802801132202\n",
      "epoch:  720  loss:  1.6675775051116943\n",
      "epoch:  721  loss:  1.6673786640167236\n",
      "epoch:  722  loss:  1.667180061340332\n",
      "epoch:  723  loss:  1.6669774055480957\n",
      "epoch:  724  loss:  1.6667752265930176\n",
      "epoch:  725  loss:  1.6665760278701782\n",
      "epoch:  726  loss:  1.666383981704712\n",
      "epoch:  727  loss:  1.6661953926086426\n",
      "epoch:  728  loss:  1.6660085916519165\n",
      "epoch:  729  loss:  1.665829062461853\n",
      "epoch:  730  loss:  1.665642261505127\n",
      "epoch:  731  loss:  1.6654545068740845\n",
      "epoch:  732  loss:  1.665256381034851\n",
      "epoch:  733  loss:  1.665054440498352\n",
      "epoch:  734  loss:  1.664852499961853\n",
      "epoch:  735  loss:  1.6646543741226196\n",
      "epoch:  736  loss:  1.6644634008407593\n",
      "epoch:  737  loss:  1.6642768383026123\n",
      "epoch:  738  loss:  1.6640892028808594\n",
      "epoch:  739  loss:  1.6638952493667603\n",
      "epoch:  740  loss:  1.663701057434082\n",
      "epoch:  741  loss:  1.6634995937347412\n",
      "epoch:  742  loss:  1.6633059978485107\n",
      "epoch:  743  loss:  1.6631174087524414\n",
      "epoch:  744  loss:  1.6629338264465332\n",
      "epoch:  745  loss:  1.6627585887908936\n",
      "epoch:  746  loss:  1.6625827550888062\n",
      "epoch:  747  loss:  1.662405252456665\n",
      "epoch:  748  loss:  1.6622234582901\n",
      "epoch:  749  loss:  1.6620389223098755\n",
      "epoch:  750  loss:  1.661851167678833\n",
      "epoch:  751  loss:  1.6616640090942383\n",
      "epoch:  752  loss:  1.6614805459976196\n",
      "epoch:  753  loss:  1.6612968444824219\n",
      "epoch:  754  loss:  1.6611160039901733\n",
      "epoch:  755  loss:  1.6609318256378174\n",
      "epoch:  756  loss:  1.6607470512390137\n",
      "epoch:  757  loss:  1.6605596542358398\n",
      "epoch:  758  loss:  1.660373330116272\n",
      "epoch:  759  loss:  1.6601896286010742\n",
      "epoch:  760  loss:  1.6600102186203003\n",
      "epoch:  761  loss:  1.6598329544067383\n",
      "epoch:  762  loss:  1.6596577167510986\n",
      "epoch:  763  loss:  1.659492015838623\n",
      "epoch:  764  loss:  1.6593250036239624\n",
      "epoch:  765  loss:  1.6591620445251465\n",
      "epoch:  766  loss:  1.6590031385421753\n",
      "epoch:  767  loss:  1.658842921257019\n",
      "epoch:  768  loss:  1.6586674451828003\n",
      "epoch:  769  loss:  1.6584874391555786\n",
      "epoch:  770  loss:  1.658300757408142\n",
      "epoch:  771  loss:  1.6581158638000488\n",
      "epoch:  772  loss:  1.6579402685165405\n",
      "epoch:  773  loss:  1.6577762365341187\n",
      "epoch:  774  loss:  1.6576176881790161\n",
      "epoch:  775  loss:  1.6574609279632568\n",
      "epoch:  776  loss:  1.6572980880737305\n",
      "epoch:  777  loss:  1.6571261882781982\n",
      "epoch:  778  loss:  1.6569504737854004\n",
      "epoch:  779  loss:  1.6567742824554443\n",
      "epoch:  780  loss:  1.6565998792648315\n",
      "epoch:  781  loss:  1.6564327478408813\n",
      "epoch:  782  loss:  1.6562713384628296\n",
      "epoch:  783  loss:  1.65610933303833\n",
      "epoch:  784  loss:  1.6559479236602783\n",
      "epoch:  785  loss:  1.6557856798171997\n",
      "epoch:  786  loss:  1.6556272506713867\n",
      "epoch:  787  loss:  1.6554646492004395\n",
      "epoch:  788  loss:  1.655306100845337\n",
      "epoch:  789  loss:  1.655144214630127\n",
      "epoch:  790  loss:  1.6549819707870483\n",
      "epoch:  791  loss:  1.6548271179199219\n",
      "epoch:  792  loss:  1.6546705961227417\n",
      "epoch:  793  loss:  1.6545177698135376\n",
      "epoch:  794  loss:  1.6543643474578857\n",
      "epoch:  795  loss:  1.6542067527770996\n",
      "epoch:  796  loss:  1.6540485620498657\n",
      "epoch:  797  loss:  1.6538832187652588\n",
      "epoch:  798  loss:  1.6537182331085205\n",
      "epoch:  799  loss:  1.6535537242889404\n",
      "epoch:  800  loss:  1.6533929109573364\n",
      "epoch:  801  loss:  1.6532347202301025\n",
      "epoch:  802  loss:  1.653085470199585\n",
      "epoch:  803  loss:  1.6529327630996704\n",
      "epoch:  804  loss:  1.6527842283248901\n",
      "epoch:  805  loss:  1.6526340246200562\n",
      "epoch:  806  loss:  1.6524845361709595\n",
      "epoch:  807  loss:  1.6523323059082031\n",
      "epoch:  808  loss:  1.6521782875061035\n",
      "epoch:  809  loss:  1.6520272493362427\n",
      "epoch:  810  loss:  1.6518731117248535\n",
      "epoch:  811  loss:  1.6517236232757568\n",
      "epoch:  812  loss:  1.6515746116638184\n",
      "epoch:  813  loss:  1.651431918144226\n",
      "epoch:  814  loss:  1.6512905359268188\n",
      "epoch:  815  loss:  1.6511508226394653\n",
      "epoch:  816  loss:  1.6510120630264282\n",
      "epoch:  817  loss:  1.6508654356002808\n",
      "epoch:  818  loss:  1.6507169008255005\n",
      "epoch:  819  loss:  1.6505619287490845\n",
      "epoch:  820  loss:  1.650406837463379\n",
      "epoch:  821  loss:  1.650252103805542\n",
      "epoch:  822  loss:  1.6501041650772095\n",
      "epoch:  823  loss:  1.6499539613723755\n",
      "epoch:  824  loss:  1.6498104333877563\n",
      "epoch:  825  loss:  1.6496700048446655\n",
      "epoch:  826  loss:  1.6495275497436523\n",
      "epoch:  827  loss:  1.6493853330612183\n",
      "epoch:  828  loss:  1.6492465734481812\n",
      "epoch:  829  loss:  1.6491022109985352\n",
      "epoch:  830  loss:  1.6489551067352295\n",
      "epoch:  831  loss:  1.6488091945648193\n",
      "epoch:  832  loss:  1.6486653089523315\n",
      "epoch:  833  loss:  1.648524522781372\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    y_pred = model(x_train_smol)\n",
    "    loss = loss_fn(y_pred, y_train_smol)\n",
    "    print('epoch: ', epoch, ' loss: ', loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_smol, y_test_smol = get_data('./data/aclImdb/test/labeledBow.feat', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.23875\n"
     ]
    }
   ],
   "source": [
    "y_pred = model(x_test_smol)\n",
    "labels_pred = torch.argmax(y_pred, 1)\n",
    "correct = (labels_pred == y_test_smol).sum().item()\n",
    "print('Accuracy: ' + str(correct / len(y_test_smol)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
