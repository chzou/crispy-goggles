{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89527"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/aclImdb/imdb.vocab', 'r') as f:\n",
    "    VOCAB_LEN = len(f.readlines())\n",
    "VOCAB_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "NUM_EXAMPLES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(n):\n",
    "    arr = np.zeros(NUM_CLASSES)\n",
    "    arr[int(n)-1] = 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_to_vec(features):\n",
    "    arr = np.zeros(VOCAB_LEN)\n",
    "    for f in features:\n",
    "        i, c = f.split(':') # index, count\n",
    "        arr[int(i)] = int(c)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_data(filename, num_examples):\n",
    "    with open(filename, 'r') as f:\n",
    "        imdb = f.readlines()\n",
    "    \n",
    "    x_train, y_train = [], []\n",
    "    label_count = defaultdict(int) # used to balance dataset\n",
    "    for line in imdb:\n",
    "        label, *features = line.split(' ')\n",
    "        if label_count[label] >= NUM_EXAMPLES / NUM_CLASSES:\n",
    "            continue\n",
    "        x_train.append(bow_to_vec(features))\n",
    "        y_train.append(int(label) - 1)\n",
    "        label_count[label] += 1\n",
    "    \n",
    "    x_train = torch.tensor(x_train, dtype=torch.float)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_smol, y_train_smol = get_data('./data/aclImdb/train/labeledBow.feat', NUM_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in, n_h, n_out = VOCAB_LEN, NUM_EXAMPLES, NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(n_in, n_h),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(n_h, n_out),\n",
    "                     nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  2.301706552505493\n",
      "epoch:  1  loss:  2.219895124435425\n",
      "epoch:  2  loss:  2.1454620361328125\n",
      "epoch:  3  loss:  2.0843961238861084\n",
      "epoch:  4  loss:  2.0191826820373535\n",
      "epoch:  5  loss:  1.9466955661773682\n",
      "epoch:  6  loss:  1.8802120685577393\n",
      "epoch:  7  loss:  1.8284648656845093\n",
      "epoch:  8  loss:  1.7799153327941895\n",
      "epoch:  9  loss:  1.7327296733856201\n",
      "epoch:  10  loss:  1.6932847499847412\n",
      "epoch:  11  loss:  1.660703182220459\n",
      "epoch:  12  loss:  1.6323150396347046\n",
      "epoch:  13  loss:  1.6071144342422485\n",
      "epoch:  14  loss:  1.585159420967102\n",
      "epoch:  15  loss:  1.5665827989578247\n",
      "epoch:  16  loss:  1.551084041595459\n",
      "epoch:  17  loss:  1.5380465984344482\n",
      "epoch:  18  loss:  1.5269041061401367\n",
      "epoch:  19  loss:  1.5173335075378418\n",
      "epoch:  20  loss:  1.5092025995254517\n",
      "epoch:  21  loss:  1.5023995637893677\n",
      "epoch:  22  loss:  1.4967471361160278\n",
      "epoch:  23  loss:  1.4920393228530884\n",
      "epoch:  24  loss:  1.4880836009979248\n",
      "epoch:  25  loss:  1.4847332239151\n",
      "epoch:  26  loss:  1.4818763732910156\n",
      "epoch:  27  loss:  1.4794325828552246\n",
      "epoch:  28  loss:  1.477339506149292\n",
      "epoch:  29  loss:  1.4755483865737915\n",
      "epoch:  30  loss:  1.4740142822265625\n",
      "epoch:  31  loss:  1.4727023839950562\n",
      "epoch:  32  loss:  1.4715808629989624\n",
      "epoch:  33  loss:  1.4706149101257324\n",
      "epoch:  34  loss:  1.4697816371917725\n",
      "epoch:  35  loss:  1.469051718711853\n",
      "epoch:  36  loss:  1.4684076309204102\n",
      "epoch:  37  loss:  1.4678356647491455\n",
      "epoch:  38  loss:  1.4673256874084473\n",
      "epoch:  39  loss:  1.466872215270996\n",
      "epoch:  40  loss:  1.4664686918258667\n",
      "epoch:  41  loss:  1.466108798980713\n",
      "epoch:  42  loss:  1.4657901525497437\n",
      "epoch:  43  loss:  1.4655040502548218\n",
      "epoch:  44  loss:  1.4652477502822876\n",
      "epoch:  45  loss:  1.4650152921676636\n",
      "epoch:  46  loss:  1.4648072719573975\n",
      "epoch:  47  loss:  1.4646168947219849\n",
      "epoch:  48  loss:  1.4644420146942139\n",
      "epoch:  49  loss:  1.4642819166183472\n",
      "epoch:  50  loss:  1.4641352891921997\n",
      "epoch:  51  loss:  1.4640018939971924\n",
      "epoch:  52  loss:  1.4638757705688477\n",
      "epoch:  53  loss:  1.4637598991394043\n",
      "epoch:  54  loss:  1.4636539220809937\n",
      "epoch:  55  loss:  1.4635542631149292\n",
      "epoch:  56  loss:  1.4634623527526855\n",
      "epoch:  57  loss:  1.4633756875991821\n",
      "epoch:  58  loss:  1.4632967710494995\n",
      "epoch:  59  loss:  1.463220238685608\n",
      "epoch:  60  loss:  1.4631503820419312\n",
      "epoch:  61  loss:  1.4630839824676514\n",
      "epoch:  62  loss:  1.4630213975906372\n",
      "epoch:  63  loss:  1.4629640579223633\n",
      "epoch:  64  loss:  1.4629075527191162\n",
      "epoch:  65  loss:  1.4628552198410034\n",
      "epoch:  66  loss:  1.462805151939392\n",
      "epoch:  67  loss:  1.4627565145492554\n",
      "epoch:  68  loss:  1.462710976600647\n",
      "epoch:  69  loss:  1.46266770362854\n",
      "epoch:  70  loss:  1.4626269340515137\n",
      "epoch:  71  loss:  1.4625874757766724\n",
      "epoch:  72  loss:  1.462548851966858\n",
      "epoch:  73  loss:  1.4625120162963867\n",
      "epoch:  74  loss:  1.462477445602417\n",
      "epoch:  75  loss:  1.4624438285827637\n",
      "epoch:  76  loss:  1.4624128341674805\n",
      "epoch:  77  loss:  1.462382197380066\n",
      "epoch:  78  loss:  1.4623521566390991\n",
      "epoch:  79  loss:  1.4623229503631592\n",
      "epoch:  80  loss:  1.4622961282730103\n",
      "epoch:  81  loss:  1.4622684717178345\n",
      "epoch:  82  loss:  1.462242841720581\n",
      "epoch:  83  loss:  1.4622182846069336\n",
      "epoch:  84  loss:  1.4621940851211548\n",
      "epoch:  85  loss:  1.4621713161468506\n",
      "epoch:  86  loss:  1.4621485471725464\n",
      "epoch:  87  loss:  1.4621267318725586\n",
      "epoch:  88  loss:  1.4621042013168335\n",
      "epoch:  89  loss:  1.4620845317840576\n",
      "epoch:  90  loss:  1.4620646238327026\n",
      "epoch:  91  loss:  1.4620457887649536\n",
      "epoch:  92  loss:  1.4620267152786255\n",
      "epoch:  93  loss:  1.4620083570480347\n",
      "epoch:  94  loss:  1.4619899988174438\n",
      "epoch:  95  loss:  1.4619724750518799\n",
      "epoch:  96  loss:  1.4619569778442383\n",
      "epoch:  97  loss:  1.4619412422180176\n",
      "epoch:  98  loss:  1.4619255065917969\n",
      "epoch:  99  loss:  1.4619096517562866\n",
      "epoch:  100  loss:  1.4618949890136719\n",
      "epoch:  101  loss:  1.4618792533874512\n",
      "epoch:  102  loss:  1.4618664979934692\n",
      "epoch:  103  loss:  1.4618523120880127\n",
      "epoch:  104  loss:  1.4618397951126099\n",
      "epoch:  105  loss:  1.4618266820907593\n",
      "epoch:  106  loss:  1.4618147611618042\n",
      "epoch:  107  loss:  1.4618017673492432\n",
      "epoch:  108  loss:  1.4617902040481567\n",
      "epoch:  109  loss:  1.461777687072754\n",
      "epoch:  110  loss:  1.4617668390274048\n",
      "epoch:  111  loss:  1.4617552757263184\n",
      "epoch:  112  loss:  1.4617440700531006\n",
      "epoch:  113  loss:  1.4617340564727783\n",
      "epoch:  114  loss:  1.4617232084274292\n",
      "epoch:  115  loss:  1.4617136716842651\n",
      "epoch:  116  loss:  1.4617035388946533\n",
      "epoch:  117  loss:  1.461694359779358\n",
      "epoch:  118  loss:  1.4616847038269043\n",
      "epoch:  119  loss:  1.4616754055023193\n",
      "epoch:  120  loss:  1.4616656303405762\n",
      "epoch:  121  loss:  1.461656928062439\n",
      "epoch:  122  loss:  1.4616484642028809\n",
      "epoch:  123  loss:  1.4616403579711914\n",
      "epoch:  124  loss:  1.461632490158081\n",
      "epoch:  125  loss:  1.4616246223449707\n",
      "epoch:  126  loss:  1.4616165161132812\n",
      "epoch:  127  loss:  1.4616092443466187\n",
      "epoch:  128  loss:  1.461601972579956\n",
      "epoch:  129  loss:  1.461594581604004\n",
      "epoch:  130  loss:  1.4615874290466309\n",
      "epoch:  131  loss:  1.461580514907837\n",
      "epoch:  132  loss:  1.461573839187622\n",
      "epoch:  133  loss:  1.4615669250488281\n",
      "epoch:  134  loss:  1.4615613222122192\n",
      "epoch:  135  loss:  1.4615542888641357\n",
      "epoch:  136  loss:  1.4615474939346313\n",
      "epoch:  137  loss:  1.461540937423706\n",
      "epoch:  138  loss:  1.4615352153778076\n",
      "epoch:  139  loss:  1.4615293741226196\n",
      "epoch:  140  loss:  1.4615235328674316\n",
      "epoch:  141  loss:  1.4615169763565063\n",
      "epoch:  142  loss:  1.4615119695663452\n",
      "epoch:  143  loss:  1.4615073204040527\n",
      "epoch:  144  loss:  1.461502194404602\n",
      "epoch:  145  loss:  1.461497187614441\n",
      "epoch:  146  loss:  1.4614924192428589\n",
      "epoch:  147  loss:  1.4614874124526978\n",
      "epoch:  148  loss:  1.461482286453247\n",
      "epoch:  149  loss:  1.461477518081665\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(150):\n",
    "    y_pred = model(x_train_smol)\n",
    "    loss = loss_fn(y_pred, y_train_smol)\n",
    "    print('epoch: ', epoch, ' loss: ', loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_smol, y_test_smol = get_data('./data/aclImdb/test/labeledBow.feat', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2375\n"
     ]
    }
   ],
   "source": [
    "y_pred = model(x_test_smol)\n",
    "labels_pred = torch.argmax(y_pred, 1)\n",
    "correct = (labels_pred == y_test_smol).sum().item()\n",
    "print('Accuracy: ' + str(correct / len(y_test_smol)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
